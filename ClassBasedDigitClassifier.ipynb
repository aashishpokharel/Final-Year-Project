{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d63ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd65f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = {\n",
    "                'digit_0' : 0,\n",
    "                'digit_1' : 1,\n",
    "                'digit_2' : 2,\n",
    "                'digit_3' : 3,\n",
    "                'digit_4' : 4,\n",
    "                'digit_5' : 5,\n",
    "                'digit_6' : 6,\n",
    "                'digit_7' : 7,\n",
    "                'digit_8' : 8,\n",
    "                'digit_9' : 9,\n",
    "}\n",
    "train_data = pd.read_csv('./dataset/train_digits_data.csv')\n",
    "test_data  = pd.read_csv('./dataset/test_digits_data.csv')\n",
    "X_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data.iloc[:, -1]\n",
    "y_train = y_train.replace(charset)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61885a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "X_train = mm.fit_transform(X_train)\n",
    "# X_dev   = mm.fit_transform(X_dev)\n",
    "X_test = mm.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5185ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f76d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Before Processing\t After Processing\n",
      "=================================================================\n",
      "Training Set Images:\t(17000, 1025)\t\t(13600, 1024)\n",
      "Training Set Labels:\t(17000,)\t\t(13600,)\n",
      "Test Set Images:\t(3000, 1025)\t\t(3400, 1024)\n",
      "Test Set Labels:\t(3000,)\t\t\t(3000,)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Data\\t\\t\\t\",\"Before Processing\\t\",\"After Processing\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(train_data.shape)+\"\\t\\t\"+ str(X_train.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(train_data.iloc[:, -1].shape)+\"\\t\\t\"+ str(y_train.shape))\n",
    "# print(\"Dev Set Images:\\t\\t\" + str(X_dev.shape)+\"\\t\\t\"+ str(X_dev.shape))\n",
    "# print(\"Dev Set Labels:\\t\\t\" + str(y_dev.shape)+\"\\t\\t\\t\"+ str(y_dev.shape))\n",
    "print(\"Test Set Images:\\t\" + str(test_data.shape)+\"\\t\\t\"+ str(X_test.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(test_data.iloc[:, -1].shape)+\"\\t\\t\\t\"+ str(test_data.iloc[:, -1].shape))\n",
    "print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a695ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier:\n",
    "    def __init__(self, n_inputs, n_neurons = [32,32,10]):\n",
    "        np.random.seed(42)\n",
    "    # We have done here n_inputs/n_neurons instead of n_neurons/n_inputs to prevent the Transpose everytime\n",
    "        self.weights1 = 0.01 * np.random.randn(n_inputs, n_neurons[0]) # The input shape and no of neurons you want to have in the layer\n",
    "        self.biases1 =  0.01 * np.random.randn(1, n_neurons[0])\n",
    "        self.weights2 = 0.01 *np.random.randn(n_neurons[0], n_neurons[1]) # The input shape and no of neurons you want to have in the layer\n",
    "        self.biases2 = 0.01 * np.random.randn(1, n_neurons[1])\n",
    "        self.weights3 = 0.01 * np.random.randn(n_neurons[1], n_neurons[2])\n",
    "        self.biases3 = 0.01 * np.random.randn(1, n_neurons[2])\n",
    "        self.output1 = None\n",
    "        self.output2 = None\n",
    "        self.output3 = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        # activatied outputs\n",
    "        self.output1_act = None\n",
    "        self.output2_act = None\n",
    "        self.output3_act = None\n",
    "    def forward(self, inputs, weights, biases):\n",
    "        \"\"\" The dot product of the input - weights - Biases (y = Wx + b) \"\"\"\n",
    "        output = np.dot(inputs, weights) + biases\n",
    "        if(np.isnan(np.sum(output))):\n",
    "            raise Exception(\"NaN values present in FW pass\")\n",
    "        elif(np.isinf(np.sum(output))):\n",
    "            raise Exception(\"INF values present in FW Pass\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def ReLU(self, inputs):\n",
    "        \"\"\" Rectified Linear Activation Function \"\"\"\n",
    "        output = np.maximum(0, inputs)\n",
    "        return output\n",
    "    \n",
    "    def Softmax(self, inputs):\n",
    "    # subtract largest value to prevent overflow\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        if(np.isnan(np.sum(probabilities))):\n",
    "            raise Exception(\"NaN values present in Softmax For\")\n",
    "        elif(np.isinf(np.sum(probabilities))):\n",
    "            raise Exception(\"INF values present in Softmax For\")\n",
    "        \n",
    "        return probabilities\n",
    "        \n",
    "    def categorical_cross_entropy(self,y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-6, 1-1e-6)\n",
    "        # Handling if labels are 1D \n",
    "        correct_confidences = None\n",
    "        if len(y_true.shape) == 1:\n",
    "#             print(y_pred_clipped[range(samples), :].shape)\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) ==2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis =1)\n",
    "        else:\n",
    "            raise Exception(\"Sorry, no numbers below zero\")\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "#         print(negative_log_likelihoods.shape)\n",
    "        return negative_log_likelihoods \n",
    "    \n",
    "    def linear_backward(self,inputs, weights, dvalues):\n",
    "        self.dweights_linear = np.dot(inputs.T, dvalues)\n",
    "        self.dbiases_linear = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinput_linear = np.dot(dvalues, weights.T)\n",
    "        \n",
    "        if(np.isnan(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"NaN values present in Linear Back\")\n",
    "        elif(np.isinf(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"INF values present in Linear BAck\")\n",
    "        \n",
    "        \n",
    "        return self.dweights_linear, self.dinput_linear\n",
    "    \n",
    "    def linear_backward_with_l2(self,inputs, weights, dvalues, lambd = 0.7):\n",
    "        \"\"\"  \"\"\"\n",
    "        m = inputs.shape[1]\n",
    "        self.dweights_linear = np.dot(inputs.T, dvalues) + (lambd*weights)/m\n",
    "        self.dbiases_linear = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinput_linear = np.dot(dvalues, weights.T) \n",
    "        \n",
    "        if(np.isnan(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"NaN values present in Linear Back\")\n",
    "        elif(np.isinf(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"INF values present in Linear BAck\")\n",
    "        \n",
    "        \n",
    "        return self.dweights_linear, self.dinput_linear\n",
    "    \n",
    "    def softmax_backward(self,dA, Z):\n",
    "        \"\"\"Compute backward pass for softmax activation\"\"\"\n",
    "        softmax_output = Softmax(Z) \n",
    "        return softmax_output * (1 - softmax_output) * dA\n",
    "\n",
    "    def ReLU_backward(self,dA, Z):\n",
    "        \n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        if(np.isnan(np.sum(dZ))):\n",
    "            raise Exception(\"NaN values present in RELU Back\")\n",
    "        elif(np.isinf(np.sum(dZ))):\n",
    "            raise Exception(\"INF values present in RELU BAck\")\n",
    "        return dZ\n",
    "        \n",
    "    def categorical_cross_entropy_backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs_loss = self.dinputs / samples\n",
    "        if(np.isnan(np.sum(self.dinputs))):\n",
    "            raise Exception(\"NaN values present in Softmax Back\")\n",
    "        elif(np.isinf(np.sum(self.dinputs))):\n",
    "            raise Exception(\"INF values present in Softmax_back\")\n",
    "        return self.dinputs\n",
    "    \n",
    "    def softmax_categorical_cross_entropy_combined_backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        #handling Ohe values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs_combined = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs_combined[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs_combined = self.dinputs_combined / samples\n",
    "        if(np.isnan(np.sum(self.dinputs_combined))):\n",
    "            raise Exception(\"NaN values present in Softmax Back\")\n",
    "        elif(np.isinf(np.sum(self.dinputs_combined))):\n",
    "            raise Exception(\"INF values present in Softmax_back\")\n",
    "       \n",
    "        return self.dinputs_combined\n",
    "        \n",
    "    \n",
    "    def compute_loss(self,y_pred, y_true):\n",
    "        sample_losses = self.categorical_cross_entropy(y_pred, y_true)\n",
    "        loss = np.mean(sample_losses)\n",
    "        return loss\n",
    "    \n",
    "    def compute_loss_with_l2(self,y_pred, y_true, lambd = 0.7):\n",
    "        m = 10\n",
    "        sample_losses = self.categorical_cross_entropy(y_pred, y_true)\n",
    "        L2_regularization_cost = (lambd/(2*m))*(np.sum(np.square(self.weights1) + np.sum(np.square(self.weights2) + np.sum(np.square(self.weights3)))))\n",
    "        loss = np.mean(sample_losses) \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        self.X = X\n",
    "        self.output1     = self.forward(self.X, self.weights1, self.biases1)\n",
    "        self.output1_act = self.ReLU(self.output1)\n",
    "        self.output2     = self.forward(self.output1_act, self.weights2, self.biases2)\n",
    "        self.output2_act = self.ReLU(self.output2)\n",
    "        self.output3     = self.forward(self.output2_act, self.weights3, self.biases3)\n",
    "        self.output3_act = self.Softmax(self.output3)\n",
    "#         print(\"Softmax SUM\", np.sum(self.output3_act, axis = 1))\n",
    "        if(np.isnan(np.sum(self.output3_act))):\n",
    "            raise Exception(\"NaN values present in data\")\n",
    "        elif(np.isinf(np.sum(self.output3_act))):\n",
    "            raise Exception(\"INF values present in data\")\n",
    "        \n",
    "        \n",
    "    def check_inf(self):\n",
    "        check_weights = np.any(np.isinf(self.weights1)) or np.any(np.isinf(self.weights2)) or np.any(np.isinf(self.weights3))\n",
    "        check_bias    = np.any(np.isinf(self.biases1)) or np.any(np.isinf(self.biases2)) or np.any(np.isinf(self.biases3))\n",
    "        return (check_weights or check_bias)\n",
    "    \n",
    "    def backward_pass(self, y, learning_rate= 0.1, iteration = 10000):\n",
    "        self.y = y\n",
    "        for i in range(iteration):\n",
    "            self.forward_pass(self.X)\n",
    "            predictions = np.argmax(self.output3_act, axis=1)\n",
    "            \n",
    "            \n",
    "            gradient_output3_act                  = self.softmax_categorical_cross_entropy_combined_backward(self.output3_act, self.y)\n",
    "            gradient_output3, gradient_input3     = self.linear_backward(self.output2,self.weights3,gradient_output3_act)\n",
    "            gradient_output2_act                  = self.ReLU_backward(gradient_input3, self.output2)\n",
    "            gradient_output2, gradient_input2     = self.linear_backward(self.output1, self.weights2, gradient_output2_act)\n",
    "            gradient_output1_act                  = self.ReLU_backward(gradient_input2, self.output1)\n",
    "            gradient_output1, gradient_input1     = self.linear_backward(self.X, self.weights1, gradient_output1_act)\n",
    "            \n",
    "            self.weights3  = self.weights3 - learning_rate * gradient_output3\n",
    "            self.weights2  = self.weights2 - learning_rate * gradient_output2\n",
    "            self.weights1  = self.weights1 - learning_rate * gradient_output1\n",
    "            assert np.sum(gradient_output1) != np.nan, \"The gradient has nan\"\n",
    "            assert np.sum(gradient_output1) != np.inf, \"The gradient has inf\"\n",
    "            if i%100 == 0:\n",
    "\n",
    "                loss = self.compute_loss(self.output3_act, y)\n",
    "                self.accuracy = np.mean(predictions==self.y)\n",
    "                if(self.accuracy > 99.0):\n",
    "                    break\n",
    "                print(f'Loss after a iteration {i}:{loss} || Accuracy: {self.accuracy * 100}')\n",
    "                \n",
    "    def backward_pass_with_l2(self, y, learning_rate= 0.1, iteration = 10000):\n",
    "        self.y = y\n",
    "        for i in range(iteration):\n",
    "            self.forward_pass(self.X)\n",
    "            predictions = np.argmax(self.output3_act, axis=1)\n",
    "            \n",
    "            \n",
    "            gradient_output3_act                  = self.softmax_categorical_cross_entropy_combined_backward(self.output3_act, self.y)\n",
    "            gradient_output3, gradient_input3     = self.linear_backward_with_l2(self.output2,self.weights3,gradient_output3_act)\n",
    "            gradient_output2_act                  = self.ReLU_backward(gradient_input3, self.output2)\n",
    "            gradient_output2, gradient_input2     = self.linear_backward_with_l2(self.output1, self.weights2, gradient_output2_act)\n",
    "            gradient_output1_act                  = self.ReLU_backward(gradient_input2, self.output1)\n",
    "            gradient_output1, gradient_input1     = self.linear_backward_with_l2(self.X, self.weights1, gradient_output1_act)\n",
    "            \n",
    "            self.weights3  = self.weights3 - learning_rate * gradient_output3\n",
    "            self.weights2  = self.weights2 - learning_rate * gradient_output2\n",
    "            self.weights1  = self.weights1 - learning_rate * gradient_output1\n",
    "            assert np.sum(gradient_output1) != np.nan, \"The gradient has nan\"\n",
    "            assert np.sum(gradient_output1) != np.inf, \"The gradient has inf\"\n",
    "            if i%100 == 0:\n",
    "\n",
    "                loss = self.compute_loss_with_l2(self.output3_act, y)\n",
    "                self.accuracy = np.mean(predictions==self.y)\n",
    "                if(self.accuracy > .98):\n",
    "                    break\n",
    "                print(f'Loss after a iteration {i}:{loss} || Accuracy: {self.accuracy * 100}')\n",
    "\n",
    "    def load_model(self, weights, biases):\n",
    "        self.weights1 = weights['1']\n",
    "        self.weights2 = weights['2']\n",
    "        self.weights3 = weights['3']\n",
    "        \n",
    "        self.biases1 = weights['b1']\n",
    "        self.biases2 = weights['b2']\n",
    "        self.biases3 = weights['b3']\n",
    "    \n",
    "    def save_model(self, filename = f'model.pkl'):\n",
    "        from datetime import date\n",
    "\n",
    "        today = date.today()\n",
    "\n",
    "        filename = f'model_{self.accuracy}-{today}.pkl'\n",
    "        weights = {\n",
    "                    '1': self.weights1, '2': self.weights2, '3': self.weights3, \n",
    "                    'b1':self.biases1,'b2':self.biases2,'b3':self.biases3\n",
    "        }\n",
    "        pickle.dump(weights, open(filename, 'wb'))\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        output1     = self.forward(X_test, self.weights1, self.biases1)\n",
    "        output1_act = self.ReLU(output1)\n",
    "        output2     = self.forward(output1_act, self.weights2, self.biases2)\n",
    "        output2_act = self.ReLU(output2)\n",
    "        output3     = self.forward(output2_act, self.weights3, self.biases3)\n",
    "        output3_act = self.Softmax(output3)\n",
    "        prediction = np.argmax(output3_act, axis=1)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f16fc187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after a iteration 0:2.30261754370017 || Accuracy: 10.117647058823529\n",
      "Loss after a iteration 100:2.3025483071076476 || Accuracy: 10.117647058823529\n",
      "Loss after a iteration 200:2.302516337984487 || Accuracy: 10.117647058823529\n",
      "Loss after a iteration 300:2.3024734366970767 || Accuracy: 10.117647058823529\n",
      "Loss after a iteration 400:2.302376162735294 || Accuracy: 10.117647058823529\n",
      "Loss after a iteration 500:2.302071982938243 || Accuracy: 10.117647058823529\n",
      "Loss after a iteration 600:2.300060397216395 || Accuracy: 10.161764705882353\n",
      "Loss after a iteration 700:2.0572952575513415 || Accuracy: 22.139705882352942\n",
      "Loss after a iteration 800:1.5486985714446624 || Accuracy: 36.02205882352941\n",
      "Loss after a iteration 900:1.4262394648473453 || Accuracy: 39.904411764705884\n",
      "Loss after a iteration 1000:1.3899276120655606 || Accuracy: 38.029411764705884\n",
      "Loss after a iteration 1100:1.3760706065052362 || Accuracy: 36.65441176470588\n",
      "Loss after a iteration 1200:1.3665237784959974 || Accuracy: 36.830882352941174\n",
      "Loss after a iteration 1300:1.3391256417244601 || Accuracy: 43.455882352941174\n",
      "Loss after a iteration 1400:0.9699711503859877 || Accuracy: 62.595588235294116\n",
      "Loss after a iteration 1500:0.8383657534116417 || Accuracy: 68.76470588235294\n",
      "Loss after a iteration 1600:0.7694456129004628 || Accuracy: 72.08823529411764\n",
      "Loss after a iteration 1700:0.6794200686551728 || Accuracy: 76.0\n",
      "Loss after a iteration 1800:0.5244566852027496 || Accuracy: 81.31617647058823\n",
      "Loss after a iteration 1900:0.45422360471107276 || Accuracy: 84.125\n",
      "Loss after a iteration 2000:0.4097893364494058 || Accuracy: 86.2720588235294\n",
      "Loss after a iteration 2100:0.3811052444280521 || Accuracy: 87.60294117647058\n",
      "Loss after a iteration 2200:0.3638552616705337 || Accuracy: 88.2279411764706\n",
      "Loss after a iteration 2300:0.3524254247063356 || Accuracy: 88.70588235294117\n",
      "Loss after a iteration 2400:0.3434238728121851 || Accuracy: 88.97058823529412\n",
      "Loss after a iteration 2500:0.33513402171355877 || Accuracy: 89.33823529411765\n",
      "Loss after a iteration 2600:0.3277430545322648 || Accuracy: 89.76470588235294\n",
      "Loss after a iteration 2700:0.32104320548242044 || Accuracy: 89.99264705882352\n",
      "Loss after a iteration 2800:0.3142148020272687 || Accuracy: 90.3235294117647\n",
      "Loss after a iteration 2900:0.3071622419678035 || Accuracy: 90.63970588235294\n",
      "Loss after a iteration 3000:0.29949917129465453 || Accuracy: 90.91911764705883\n",
      "Loss after a iteration 3100:0.2910364691356411 || Accuracy: 91.23529411764706\n",
      "Loss after a iteration 3200:0.2818908002129977 || Accuracy: 91.52941176470588\n",
      "Loss after a iteration 3300:0.274710302818529 || Accuracy: 91.88970588235294\n",
      "Loss after a iteration 3400:0.27100614946333235 || Accuracy: 91.93382352941177\n",
      "Loss after a iteration 3500:0.25128581714228626 || Accuracy: 93.05882352941175\n",
      "Loss after a iteration 3600:0.35350157436140695 || Accuracy: 87.24264705882354\n",
      "Loss after a iteration 3700:0.23591234006063547 || Accuracy: 93.91176470588235\n",
      "Loss after a iteration 3800:0.2302397462388277 || Accuracy: 94.03676470588235\n",
      "Loss after a iteration 3900:0.22893152574157727 || Accuracy: 94.03676470588235\n",
      "Loss after a iteration 4000:0.41649260142158023 || Accuracy: 85.1985294117647\n",
      "Loss after a iteration 4100:0.22368560173361654 || Accuracy: 94.22058823529412\n",
      "Loss after a iteration 4200:0.4219156878582052 || Accuracy: 85.06617647058825\n",
      "Loss after a iteration 4300:0.22014127144386378 || Accuracy: 94.31617647058823\n",
      "Loss after a iteration 4400:0.22206292493313845 || Accuracy: 94.125\n",
      "Loss after a iteration 4500:0.2178954652427197 || Accuracy: 94.35294117647058\n",
      "Loss after a iteration 4600:0.21478754673221775 || Accuracy: 94.49264705882354\n",
      "Loss after a iteration 4700:0.2781836527836492 || Accuracy: 91.01470588235294\n",
      "Loss after a iteration 4800:0.21284941979117983 || Accuracy: 94.46323529411764\n",
      "Loss after a iteration 4900:0.21085335959384297 || Accuracy: 94.53676470588235\n",
      "Loss after a iteration 5000:0.3551005410860167 || Accuracy: 87.44117647058823\n",
      "Loss after a iteration 5100:0.2098703454245698 || Accuracy: 94.5514705882353\n",
      "Loss after a iteration 5200:0.20772642647601422 || Accuracy: 94.66911764705883\n",
      "Loss after a iteration 5300:0.22414242622042868 || Accuracy: 93.40441176470588\n",
      "Loss after a iteration 5400:0.2086051526522717 || Accuracy: 94.58088235294117\n",
      "Loss after a iteration 5500:0.2055138365585496 || Accuracy: 94.72794117647058\n",
      "Loss after a iteration 5600:0.2039521175088573 || Accuracy: 94.78676470588235\n",
      "Loss after a iteration 5700:0.20491203045886838 || Accuracy: 94.71323529411765\n",
      "Loss after a iteration 5800:0.20625607669111568 || Accuracy: 94.61764705882352\n",
      "Loss after a iteration 5900:0.2021615252725414 || Accuracy: 94.82352941176471\n",
      "Loss after a iteration 6000:0.20051098419061705 || Accuracy: 94.91176470588235\n",
      "Loss after a iteration 6100:0.20041171710604078 || Accuracy: 94.83088235294119\n",
      "Loss after a iteration 6200:0.20807494601541135 || Accuracy: 94.52941176470588\n",
      "Loss after a iteration 6300:0.1992074967494176 || Accuracy: 94.88970588235294\n",
      "Loss after a iteration 6400:0.1975645936139031 || Accuracy: 94.96323529411764\n",
      "Loss after a iteration 6500:0.19644426345753585 || Accuracy: 95.0\n",
      "Loss after a iteration 6600:0.1954093814769443 || Accuracy: 95.0\n",
      "Loss after a iteration 6700:0.24294250437802106 || Accuracy: 91.9264705882353\n",
      "Loss after a iteration 6800:0.19764842660000886 || Accuracy: 94.78676470588235\n",
      "Loss after a iteration 6900:0.19427674982936752 || Accuracy: 95.0\n",
      "Loss after a iteration 7000:0.19263039724115785 || Accuracy: 95.0735294117647\n",
      "Loss after a iteration 7100:0.19156517633924033 || Accuracy: 95.09558823529412\n",
      "Loss after a iteration 7200:0.2017716113521058 || Accuracy: 94.4779411764706\n",
      "Loss after a iteration 7300:0.19374449671249475 || Accuracy: 94.94117647058825\n",
      "Loss after a iteration 7400:0.19029425521122975 || Accuracy: 95.11764705882354\n",
      "Loss after a iteration 7500:0.18876354577132565 || Accuracy: 95.13235294117646\n",
      "Loss after a iteration 7600:0.18772183043089685 || Accuracy: 95.20588235294117\n",
      "Loss after a iteration 7700:0.18743195768749976 || Accuracy: 95.20588235294117\n",
      "Loss after a iteration 7800:0.19987274448512515 || Accuracy: 94.52941176470588\n",
      "Loss after a iteration 7900:0.1868672705633719 || Accuracy: 95.23529411764706\n",
      "Loss after a iteration 8000:0.18518633388771547 || Accuracy: 95.36029411764706\n",
      "Loss after a iteration 8100:0.1842151578379384 || Accuracy: 95.43382352941177\n",
      "Loss after a iteration 8200:0.1834983951985483 || Accuracy: 95.43382352941177\n",
      "Loss after a iteration 8300:0.1828521001782197 || Accuracy: 95.45588235294117\n",
      "Loss after a iteration 8400:0.18244280106282423 || Accuracy: 95.4779411764706\n",
      "Loss after a iteration 8500:0.1845574899511206 || Accuracy: 95.30882352941177\n",
      "Loss after a iteration 8600:0.2991390628305611 || Accuracy: 90.43382352941177\n",
      "Loss after a iteration 8700:0.18231971099579364 || Accuracy: 95.52941176470588\n",
      "Loss after a iteration 8800:0.18057021415496094 || Accuracy: 95.54411764705883\n",
      "Loss after a iteration 8900:0.17973829234677824 || Accuracy: 95.58088235294117\n",
      "Loss after a iteration 9000:0.17911060227328976 || Accuracy: 95.61029411764706\n",
      "Loss after a iteration 9100:0.178541618042467 || Accuracy: 95.625\n",
      "Loss after a iteration 9200:0.1779556223301858 || Accuracy: 95.6470588235294\n",
      "Loss after a iteration 9300:0.17868344532230604 || Accuracy: 95.4779411764706\n",
      "Loss after a iteration 9400:0.44266350871410054 || Accuracy: 85.94852941176471\n",
      "Loss after a iteration 9500:0.1807121394643171 || Accuracy: 95.3970588235294\n",
      "Loss after a iteration 9600:0.27533238632361734 || Accuracy: 91.30147058823529\n",
      "Loss after a iteration 9700:0.17896806982216928 || Accuracy: 95.54411764705883\n",
      "Loss after a iteration 9800:0.1767226772781093 || Accuracy: 95.68382352941175\n",
      "Loss after a iteration 9900:0.17712771527076948 || Accuracy: 95.63970588235294\n"
     ]
    }
   ],
   "source": [
    "model  = Classifier(1024, [32,16, 10])\n",
    "model.forward_pass(X_train)\n",
    "loss = model.compute_loss_with_l2(model.output3_act, y_train)\n",
    "model.backward_pass_with_l2(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d462ed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after a iteration 0:2.30261754370017 || Accuracy: 10.117647058823529\n",
      "Loss after a iteration 100:2.302487261894005 || Accuracy: 10.117647058823529\n",
      "Loss after a iteration 200:2.3021480628048643 || Accuracy: 10.117647058823529\n",
      "Loss after a iteration 300:2.2996601828648124 || Accuracy: 10.117647058823529\n",
      "Loss after a iteration 400:1.9036562366272742 || Accuracy: 25.727941176470587\n",
      "Loss after a iteration 500:1.220719935054839 || Accuracy: 48.92647058823529\n",
      "Loss after a iteration 600:0.8454581366874571 || Accuracy: 67.38970588235294\n",
      "Loss after a iteration 700:0.5299248718679525 || Accuracy: 80.55882352941175\n",
      "Loss after a iteration 800:0.45762754066040784 || Accuracy: 83.81617647058823\n",
      "Loss after a iteration 900:0.38469239891545026 || Accuracy: 87.29411764705883\n",
      "Loss after a iteration 1000:0.3207599823982182 || Accuracy: 89.93382352941175\n",
      "Loss after a iteration 1100:0.27297384399555563 || Accuracy: 91.58823529411765\n",
      "Loss after a iteration 1200:0.2350000138464463 || Accuracy: 92.79411764705883\n",
      "Loss after a iteration 1300:0.2044960145473084 || Accuracy: 93.77941176470588\n",
      "Loss after a iteration 1400:0.18195885292885697 || Accuracy: 94.72058823529412\n",
      "Loss after a iteration 1500:0.16222811923498642 || Accuracy: 95.48529411764706\n",
      "Loss after a iteration 1600:0.1457643975031529 || Accuracy: 96.01470588235294\n",
      "Loss after a iteration 1700:0.13343801021798 || Accuracy: 96.32352941176471\n",
      "Loss after a iteration 1800:0.12111253863600252 || Accuracy: 96.68382352941177\n",
      "Loss after a iteration 1900:0.11129499281710242 || Accuracy: 96.94852941176471\n",
      "Loss after a iteration 2000:0.1033382682810162 || Accuracy: 97.21323529411765\n",
      "Loss after a iteration 2100:0.09588302548466954 || Accuracy: 97.49264705882354\n",
      "Loss after a iteration 2200:0.08895734607227324 || Accuracy: 97.71323529411765\n",
      "Loss after a iteration 2300:0.08275503430133005 || Accuracy: 97.97058823529412\n",
      "Loss after a iteration 2400:0.0778038607382529 || Accuracy: 98.1470588235294\n",
      "Loss after a iteration 2500:0.07255831325398641 || Accuracy: 98.27941176470588\n",
      "Loss after a iteration 2600:0.06736604136852141 || Accuracy: 98.41176470588235\n",
      "Loss after a iteration 2700:0.06457635001947624 || Accuracy: 98.50735294117648\n",
      "Loss after a iteration 2800:0.05980095058313108 || Accuracy: 98.6470588235294\n",
      "Loss after a iteration 2900:0.05714351908144206 || Accuracy: 98.69117647058823\n",
      "Loss after a iteration 3000:0.05372591563509382 || Accuracy: 98.8529411764706\n",
      "Loss after a iteration 3100:0.05000662501826825 || Accuracy: 98.99264705882352\n",
      "Loss after a iteration 3200:0.04963324082082159 || Accuracy: 98.94117647058823\n",
      "Loss after a iteration 3300:0.04471486928173807 || Accuracy: 99.18382352941177\n",
      "Loss after a iteration 3400:0.044712784758111905 || Accuracy: 99.15441176470588\n",
      "Loss after a iteration 3500:0.040352412872069095 || Accuracy: 99.30882352941175\n",
      "Loss after a iteration 3600:0.03855594998058411 || Accuracy: 99.36029411764706\n",
      "Loss after a iteration 3700:0.036843901703385286 || Accuracy: 99.375\n",
      "Loss after a iteration 3800:0.03447426847982293 || Accuracy: 99.47058823529412\n",
      "Loss after a iteration 3900:0.03334035002200121 || Accuracy: 99.50735294117648\n",
      "Loss after a iteration 4000:0.031246265645470204 || Accuracy: 99.55882352941177\n",
      "Loss after a iteration 4100:0.030015414532363983 || Accuracy: 99.55882352941177\n",
      "Loss after a iteration 4200:0.0283637228407055 || Accuracy: 99.61764705882354\n",
      "Loss after a iteration 4300:0.026697879839351565 || Accuracy: 99.66911764705883\n",
      "Loss after a iteration 4400:0.02551224796038243 || Accuracy: 99.68382352941177\n",
      "Loss after a iteration 4500:0.024122133380981008 || Accuracy: 99.70588235294117\n",
      "Loss after a iteration 4600:0.022985200130658703 || Accuracy: 99.72058823529412\n",
      "Loss after a iteration 4700:0.02179710440917834 || Accuracy: 99.75\n",
      "Loss after a iteration 4800:0.02070517181321737 || Accuracy: 99.76470588235294\n",
      "Loss after a iteration 4900:0.019749484577009813 || Accuracy: 99.77941176470588\n",
      "Loss after a iteration 5000:0.01878542376275961 || Accuracy: 99.79411764705883\n",
      "Loss after a iteration 5100:0.017900520069055677 || Accuracy: 99.81617647058823\n",
      "Loss after a iteration 5200:0.017044632316566364 || Accuracy: 99.83088235294117\n",
      "Loss after a iteration 5300:0.016232121024422088 || Accuracy: 99.83823529411765\n",
      "Loss after a iteration 5400:0.01548245054494849 || Accuracy: 99.88235294117646\n",
      "Loss after a iteration 5500:0.014750178481044816 || Accuracy: 99.88970588235294\n",
      "Loss after a iteration 5600:0.01407261772882183 || Accuracy: 99.89705882352942\n",
      "Loss after a iteration 5700:0.013436819583859038 || Accuracy: 99.91176470588236\n",
      "Loss after a iteration 5800:0.012823663092896892 || Accuracy: 99.91176470588236\n",
      "Loss after a iteration 5900:0.01225192644424199 || Accuracy: 99.91176470588236\n",
      "Loss after a iteration 6000:0.01170992758045435 || Accuracy: 99.92647058823529\n",
      "Loss after a iteration 6100:0.011198564366632273 || Accuracy: 99.94117647058823\n",
      "Loss after a iteration 6200:0.010714188513319099 || Accuracy: 99.94117647058823\n",
      "Loss after a iteration 6300:0.010252901520193388 || Accuracy: 99.94852941176471\n",
      "Loss after a iteration 6400:0.00981445255160316 || Accuracy: 99.95588235294117\n",
      "Loss after a iteration 6500:0.00940014758712609 || Accuracy: 99.96323529411765\n",
      "Loss after a iteration 6600:0.009006723970330947 || Accuracy: 99.97058823529412\n",
      "Loss after a iteration 6700:0.008634418859551666 || Accuracy: 99.97058823529412\n",
      "Loss after a iteration 6800:0.008282520344229894 || Accuracy: 99.97058823529412\n",
      "Loss after a iteration 6900:0.007948344690450148 || Accuracy: 99.97794117647058\n",
      "Loss after a iteration 7000:0.007633829090483904 || Accuracy: 99.97794117647058\n",
      "Loss after a iteration 7100:0.007335769572823585 || Accuracy: 99.97794117647058\n",
      "Loss after a iteration 7200:0.007055220292062144 || Accuracy: 99.97794117647058\n",
      "Loss after a iteration 7300:0.006789580702234032 || Accuracy: 99.97794117647058\n",
      "Loss after a iteration 7400:0.0065386324445779525 || Accuracy: 99.97794117647058\n",
      "Loss after a iteration 7500:0.006302065920885134 || Accuracy: 99.98529411764706\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mforward_pass(X_train)\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_loss_with_l2(model\u001b[38;5;241m.\u001b[39moutput3_act, y_train)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36mClassifier.backward_pass\u001b[0;34m(self, y, learning_rate, iteration)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iteration):\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput3_act, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    193\u001b[0m     gradient_output3_act                  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax_categorical_cross_entropy_combined_backward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput3_act, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my)\n",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36mClassifier.forward_pass\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_pass\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput1     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbiases1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput1_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput1)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput2     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput1_act, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases2)\n",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[0;34m(self, inputs, weights, biases)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, weights, biases):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" The dot product of the input - weights - Biases (y = Wx + b) \"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m biases\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(np\u001b[38;5;241m.\u001b[39misnan(np\u001b[38;5;241m.\u001b[39msum(output))):\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN values present in FW pass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model  = Classifier(1024, [32,16, 10])\n",
    "model.forward_pass(X_train)\n",
    "loss = model.compute_loss_with_l2(model.output3_act, y_train)\n",
    "model.backward_pass(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "759f6c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after a iteration 0:2.302677164824908 || Accuracy: 9.88970588235294\n",
      "Loss after a iteration 100:2.3025389759383508 || Accuracy: 9.88970588235294\n",
      "Loss after a iteration 200:2.302351830029192 || Accuracy: 9.88970588235294\n",
      "Loss after a iteration 300:2.3017235407476075 || Accuracy: 9.88970588235294\n",
      "Loss after a iteration 400:2.2929413144350352 || Accuracy: 21.404411764705884\n",
      "Loss after a iteration 500:1.6143845966005945 || Accuracy: 29.772058823529413\n",
      "Loss after a iteration 600:1.423897904104247 || Accuracy: 41.61764705882353\n",
      "Loss after a iteration 700:0.8854916359191214 || Accuracy: 66.31617647058825\n",
      "Loss after a iteration 800:0.6372958570952105 || Accuracy: 76.52941176470588\n",
      "Loss after a iteration 900:0.5454960065193428 || Accuracy: 79.02941176470588\n",
      "Loss after a iteration 1000:0.5032702979940298 || Accuracy: 80.83088235294117\n",
      "Loss after a iteration 1100:0.47472469876328915 || Accuracy: 83.41911764705883\n",
      "Loss after a iteration 1200:0.43057262951013375 || Accuracy: 86.66176470588235\n",
      "Loss after a iteration 1300:0.3828469975821947 || Accuracy: 88.78676470588235\n",
      "Loss after a iteration 1400:0.35975827185091636 || Accuracy: 89.40441176470588\n",
      "Loss after a iteration 1500:0.3470982703674689 || Accuracy: 89.79411764705883\n",
      "Loss after a iteration 1600:0.33681875953964996 || Accuracy: 90.13970588235294\n",
      "Loss after a iteration 1700:0.3279748757707646 || Accuracy: 90.4264705882353\n",
      "Loss after a iteration 1800:0.31908318847981276 || Accuracy: 90.81617647058823\n",
      "Loss after a iteration 1900:0.30989515146194346 || Accuracy: 91.05882352941177\n",
      "Loss after a iteration 2000:0.29929196663209734 || Accuracy: 91.54411764705883\n",
      "Loss after a iteration 2100:0.2874617624111387 || Accuracy: 92.07352941176471\n",
      "Loss after a iteration 2200:0.2763098060414592 || Accuracy: 92.36764705882354\n",
      "Loss after a iteration 2300:0.26694142469572546 || Accuracy: 92.63970588235294\n",
      "Loss after a iteration 2400:0.2587586295641177 || Accuracy: 92.84558823529412\n",
      "Loss after a iteration 2500:0.2517297910854554 || Accuracy: 93.125\n",
      "Loss after a iteration 2600:0.244999777925549 || Accuracy: 93.32352941176471\n",
      "Loss after a iteration 2700:0.23850967340352935 || Accuracy: 93.5514705882353\n",
      "Loss after a iteration 2800:0.23275451897443966 || Accuracy: 93.79411764705883\n",
      "Loss after a iteration 2900:0.22869561034384447 || Accuracy: 93.86764705882354\n",
      "Loss after a iteration 3000:0.22802265796044383 || Accuracy: 93.90441176470588\n",
      "Loss after a iteration 3100:0.2405705075244186 || Accuracy: 93.15441176470588\n",
      "Loss after a iteration 3200:0.21092529334526344 || Accuracy: 94.38970588235294\n",
      "Loss after a iteration 3300:0.2079088401130079 || Accuracy: 94.4779411764706\n",
      "Loss after a iteration 3400:0.2053395217460258 || Accuracy: 94.5735294117647\n",
      "Loss after a iteration 3500:0.20301777959421863 || Accuracy: 94.61764705882352\n",
      "Loss after a iteration 3600:0.20143485759737093 || Accuracy: 94.63970588235294\n",
      "Loss after a iteration 3700:0.20091197534208002 || Accuracy: 94.69852941176471\n",
      "Loss after a iteration 3800:0.32679981255391527 || Accuracy: 88.41176470588236\n",
      "Loss after a iteration 3900:0.19435445212797603 || Accuracy: 94.91176470588235\n",
      "Loss after a iteration 4000:0.19266035080626032 || Accuracy: 95.0220588235294\n",
      "Loss after a iteration 4100:0.1913428537325238 || Accuracy: 95.05882352941177\n",
      "Loss after a iteration 4200:0.19036458884939508 || Accuracy: 95.0735294117647\n",
      "Loss after a iteration 4300:0.3695286788519291 || Accuracy: 87.58088235294117\n",
      "Loss after a iteration 4400:0.1862380149707637 || Accuracy: 95.23529411764706\n",
      "Loss after a iteration 4500:0.18453319062073648 || Accuracy: 95.27205882352942\n",
      "Loss after a iteration 4600:0.18334981885411586 || Accuracy: 95.32352941176471\n",
      "Loss after a iteration 4700:0.18390251001814817 || Accuracy: 95.31617647058823\n",
      "Loss after a iteration 4800:0.18008765864575782 || Accuracy: 95.36029411764706\n",
      "Loss after a iteration 4900:0.17852427084990227 || Accuracy: 95.44117647058825\n",
      "Loss after a iteration 5000:0.17736345961969846 || Accuracy: 95.4779411764706\n",
      "Loss after a iteration 5100:0.17716972691344401 || Accuracy: 95.5\n",
      "Loss after a iteration 5200:0.18398154016721738 || Accuracy: 94.79411764705883\n",
      "Loss after a iteration 5300:0.17301654951488807 || Accuracy: 95.6029411764706\n",
      "Loss after a iteration 5400:0.17201830811080654 || Accuracy: 95.63970588235294\n",
      "Loss after a iteration 5500:0.17138867683574902 || Accuracy: 95.67647058823529\n",
      "Loss after a iteration 5600:0.30184220202294443 || Accuracy: 89.01470588235294\n",
      "Loss after a iteration 5700:0.16848475554092388 || Accuracy: 95.69117647058823\n",
      "Loss after a iteration 5800:0.16774606960522756 || Accuracy: 95.75\n",
      "Loss after a iteration 5900:0.1673255313274211 || Accuracy: 95.78676470588235\n"
     ]
    }
   ],
   "source": [
    "model  = Classifier(1024, [32,32, 10])\n",
    "model.forward_pass(X_train)\n",
    "loss = model.compute_loss_with_l2(model.output3_act, y_train)\n",
    "model.backward_pass_with_l2(y_train, iteration = 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a80cde9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model.save_model(f'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8268bf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model: 88.17647058823529\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "y_acc = np.mean((y_test == y_test_pred))\n",
    "print(f'Test Accuracy of the model: {y_acc * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ffe91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 6, ..., 6, 4, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7892a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'1': model.weights1, '2': model.weights2, '3': model.weights3, \n",
    "           'b1':model.biases1,'b2':model.biases1,'b3':model.biases1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cbcf294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'base_model_weights_88.pkl'\n",
    "pickle.dump(weights, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c00ff0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'base_model_100.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daecd9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 2 6 ... 6 4 3]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.predict(X_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997142e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6402f6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "afb27453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "# foo = Image.open('dataset/Test/digit_0/103277.png')  # My image is a 200x374 jpeg that is 102kb large\n",
    "foo = Image.open('number-5.png')\n",
    "foo= foo.convert('L')\n",
    "foo = foo.resize((32,32))\n",
    "# print(foo.size)  # (200, 374)\n",
    "image = np.array(foo)\n",
    "\n",
    "image = image.reshape(-1)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16ba2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Image.fromarray(image)\n",
    "      \n",
    "    # saving the final output \n",
    "    # as a PNG file\n",
    "data.save('gfg_dummy_pic.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13f74b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea18d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastapi import FastAPI, File\n",
    "# from fastapi.middleware.cors import CORSMiddleware\n",
    "# from PIL import Image\n",
    "# import io\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import uvicorn\n",
    "# # import Classifier\n",
    "\n",
    "# app = FastAPI()\n",
    "# origins = [\n",
    "#     'http://localhost:8000',\n",
    "# ]\n",
    "# model = Classifier(1024, [32,32,10])\n",
    "# filename = 'base_model.pkl'\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# # print(loaded_model.weights1)\n",
    "# result = loaded_model.predict(X_test)\n",
    "\n",
    "# @app.get(\"/\")\n",
    "# async def root():\n",
    "#     return {\"message\": \"Wrong Method\"}\n",
    "\n",
    "# @app.post(\"/image\")\n",
    "# async def upload(file: bytes = File(...)):\n",
    "#     print(result)\n",
    "#     image = Image.open(io.BytesIO(file))\n",
    "# #     image.show()\n",
    "#     image = np.array(image)\n",
    "# #     image = image.resize((32, 32))\n",
    "# #     image = image.reshape(-1,)\n",
    "#     print(image.shape)\n",
    "#     # result = model.predict(image)\n",
    "#     print(\"The result is :\", result)\n",
    "#     return {\"Upload Status\": \"Complete\"}\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     config = uvicorn.Config(app)\n",
    "#     server = uvicorn.Server(config)\n",
    "#     await server.serve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc7772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
