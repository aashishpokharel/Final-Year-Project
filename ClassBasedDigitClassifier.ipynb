{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d63ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd65f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = {\n",
    "                'digit_0' : 0,\n",
    "                'digit_1' : 1,\n",
    "                'digit_2' : 2,\n",
    "                'digit_3' : 3,\n",
    "                'digit_4' : 4,\n",
    "                'digit_5' : 5,\n",
    "                'digit_6' : 6,\n",
    "                'digit_7' : 7,\n",
    "                'digit_8' : 8,\n",
    "                'digit_9' : 9,\n",
    "}\n",
    "train_data = pd.read_csv('./dataset/digit_all_augmented.csv')\n",
    "test_data  = pd.read_csv('./dataset/test_digits_data.csv')\n",
    "X_train = train_data.iloc[:, 1:-1].values\n",
    "y_train = train_data.iloc[:, -1]\n",
    "y_train = y_train.replace(charset)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61885a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "mm = StandardScaler()\n",
    "X_train = mm.fit_transform(X_train)\n",
    "# X_dev   = mm.fit_transform(X_dev)\n",
    "X_test = mm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038dee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd5185ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mm,open('scaler_norm.pkl', 'wb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f76d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Before Processing\t After Processing\n",
      "=================================================================\n",
      "Training Set Images:\t(85000, 1026)\t\t(68000, 1024)\n",
      "Training Set Labels:\t(85000,)\t\t(68000,)\n",
      "Test Set Images:\t(3000, 1025)\t\t(17000, 1024)\n",
      "Test Set Labels:\t(3000,)\t\t\t(3000,)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Data\\t\\t\\t\",\"Before Processing\\t\",\"After Processing\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(train_data.shape)+\"\\t\\t\"+ str(X_train.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(train_data.iloc[:, -1].shape)+\"\\t\\t\"+ str(y_train.shape))\n",
    "# print(\"Dev Set Images:\\t\\t\" + str(X_dev.shape)+\"\\t\\t\"+ str(X_dev.shape))\n",
    "# print(\"Dev Set Labels:\\t\\t\" + str(y_dev.shape)+\"\\t\\t\\t\"+ str(y_dev.shape))\n",
    "print(\"Test Set Images:\\t\" + str(test_data.shape)+\"\\t\\t\"+ str(X_test.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(test_data.iloc[:, -1].shape)+\"\\t\\t\\t\"+ str(test_data.iloc[:, -1].shape))\n",
    "print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a695ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier:\n",
    "    def __init__(self, n_inputs, n_neurons = [32,32,10]):\n",
    "        np.random.seed(42)\n",
    "    # We have done here n_inputs/n_neurons instead of n_neurons/n_inputs to prevent the Transpose everytime\n",
    "        self.weights1 = 0.01 * np.random.randn(n_inputs, n_neurons[0]) # The input shape and no of neurons you want to have in the layer\n",
    "        self.biases1 =  0.01 * np.random.randn(1, n_neurons[0])\n",
    "        self.weights2 = 0.01 *np.random.randn(n_neurons[0], n_neurons[1]) # The input shape and no of neurons you want to have in the layer\n",
    "        self.biases2 = 0.01 * np.random.randn(1, n_neurons[1])\n",
    "        self.weights3 = 0.01 * np.random.randn(n_neurons[1], n_neurons[2])\n",
    "        self.biases3 = 0.01 * np.random.randn(1, n_neurons[2])\n",
    "        self.output1 = None\n",
    "        self.output2 = None\n",
    "        self.output3 = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        # activatied outputs\n",
    "        self.output1_act = None\n",
    "        self.output2_act = None\n",
    "        self.output3_act = None\n",
    "    def forward(self, inputs, weights, biases):\n",
    "        \"\"\" The dot product of the input - weights - Biases (y = Wx + b) \"\"\"\n",
    "        output = np.dot(inputs, weights) + biases\n",
    "        if(np.isnan(np.sum(output))):\n",
    "            raise Exception(\"NaN values present in FW pass\")\n",
    "        elif(np.isinf(np.sum(output))):\n",
    "            raise Exception(\"INF values present in FW Pass\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def ReLU(self, inputs):\n",
    "        \"\"\" Rectified Linear Activation Function \"\"\"\n",
    "        output = np.maximum(0, inputs)\n",
    "        return output\n",
    "    \n",
    "    def Softmax(self, inputs):\n",
    "    # subtract largest value to prevent overflow\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        if(np.isnan(np.sum(probabilities))):\n",
    "            raise Exception(\"NaN values present in Softmax For\")\n",
    "        elif(np.isinf(np.sum(probabilities))):\n",
    "            raise Exception(\"INF values present in Softmax For\")\n",
    "        \n",
    "        return probabilities\n",
    "        \n",
    "    def categorical_cross_entropy(self,y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-6, 1-1e-6)\n",
    "        # Handling if labels are 1D \n",
    "        correct_confidences = None\n",
    "        if len(y_true.shape) == 1:\n",
    "#             print(y_pred_clipped[range(samples), :].shape)\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) ==2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis =1)\n",
    "        else:\n",
    "            raise Exception(\"Sorry, no numbers below zero\")\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "#         print(negative_log_likelihoods.shape)\n",
    "        return negative_log_likelihoods \n",
    "    \n",
    "    def linear_backward(self,inputs, weights, dvalues):\n",
    "        self.dweights_linear = np.dot(inputs.T, dvalues)\n",
    "        self.dbiases_linear = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinput_linear = np.dot(dvalues, weights.T)\n",
    "        \n",
    "        if(np.isnan(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"NaN values present in Linear Back\")\n",
    "        elif(np.isinf(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"INF values present in Linear BAck\")\n",
    "        \n",
    "        \n",
    "        return self.dweights_linear, self.dinput_linear\n",
    "    \n",
    "    def linear_backward_with_l2(self,inputs, weights, dvalues, lambd = 0.5):\n",
    "        \"\"\"  \"\"\"\n",
    "        m = inputs.shape[1]\n",
    "        self.dweights_linear = np.dot(inputs.T, dvalues) + (lambd*weights)/m\n",
    "        self.dbiases_linear = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinput_linear = np.dot(dvalues, weights.T) \n",
    "        \n",
    "        if(np.isnan(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"NaN values present in Linear Back\")\n",
    "        elif(np.isinf(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"INF values present in Linear BAck\")\n",
    "        \n",
    "        \n",
    "        return self.dweights_linear, self.dinput_linear\n",
    "    \n",
    "    def softmax_backward(self,dA, Z):\n",
    "        \"\"\"Compute backward pass for softmax activation\"\"\"\n",
    "        softmax_output = self.Softmax(Z) \n",
    "        return softmax_output * (1 - softmax_output) * dA\n",
    "\n",
    "    def ReLU_backward(self,dA, Z):\n",
    "        \n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        if(np.isnan(np.sum(dZ))):\n",
    "            raise Exception(\"NaN values present in RELU Back\")\n",
    "        elif(np.isinf(np.sum(dZ))):\n",
    "            raise Exception(\"INF values present in RELU BAck\")\n",
    "        return dZ\n",
    "        \n",
    "    def categorical_cross_entropy_backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs_loss = self.dinputs / samples\n",
    "        if(np.isnan(np.sum(self.dinputs))):\n",
    "            raise Exception(\"NaN values present in Softmax Back\")\n",
    "        elif(np.isinf(np.sum(self.dinputs))):\n",
    "            raise Exception(\"INF values present in Softmax_back\")\n",
    "        return self.dinputs\n",
    "    \n",
    "    def softmax_categorical_cross_entropy_combined_backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        #handling Ohe values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs_combined = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs_combined[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs_combined = self.dinputs_combined / samples\n",
    "        if(np.isnan(np.sum(self.dinputs_combined))):\n",
    "            raise Exception(\"NaN values present in Softmax Back\")\n",
    "        elif(np.isinf(np.sum(self.dinputs_combined))):\n",
    "            raise Exception(\"INF values present in Softmax_back\")\n",
    "       \n",
    "        return self.dinputs_combined\n",
    "        \n",
    "    \n",
    "    def compute_loss(self,y_pred, y_true):\n",
    "        sample_losses = self.categorical_cross_entropy(y_pred, y_true)\n",
    "        loss = np.mean(sample_losses)\n",
    "        return loss\n",
    "    \n",
    "    def compute_loss_with_l2(self,y_pred, y_true, lambd = 0.5):\n",
    "        m = 10\n",
    "        sample_losses = self.categorical_cross_entropy(y_pred, y_true)\n",
    "        L2_regularization_cost = (lambd/(2*m))*(np.sum(np.square(self.weights1) + np.sum(np.square(self.weights2) + np.sum(np.square(self.weights3)))))\n",
    "        loss = np.mean(sample_losses) \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        self.X = X\n",
    "        self.output1     = self.forward(self.X, self.weights1, self.biases1)\n",
    "        self.output1_act = self.ReLU(self.output1)\n",
    "        self.output2     = self.forward(self.output1_act, self.weights2, self.biases2)\n",
    "        self.output2_act = self.ReLU(self.output2)\n",
    "        self.output3     = self.forward(self.output2_act, self.weights3, self.biases3)\n",
    "        self.output3_act = self.Softmax(self.output3)\n",
    "#         print(\"Softmax SUM\", np.sum(self.output3_act, axis = 1))\n",
    "        if(np.isnan(np.sum(self.output3_act))):\n",
    "            raise Exception(\"NaN values present in data\")\n",
    "        elif(np.isinf(np.sum(self.output3_act))):\n",
    "            raise Exception(\"INF values present in data\")\n",
    "        \n",
    "        \n",
    "    def check_inf(self):\n",
    "        check_weights = np.any(np.isinf(self.weights1)) or np.any(np.isinf(self.weights2)) or np.any(np.isinf(self.weights3))\n",
    "        check_bias    = np.any(np.isinf(self.biases1)) or np.any(np.isinf(self.biases2)) or np.any(np.isinf(self.biases3))\n",
    "        return (check_weights or check_bias)\n",
    "    \n",
    "    def bgd(self, X, y, learning_rate= 0.1, iteration = 10000):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        for i in range(iteration):\n",
    "            self.forward_pass(self.X)\n",
    "            predictions = np.argmax(self.output3_act, axis=1)\n",
    "            \n",
    "            \n",
    "            gradient_output3_act                  = self.softmax_categorical_cross_entropy_combined_backward(self.output3_act, self.y)\n",
    "            gradient_output3, gradient_input3     = self.linear_backward(self.output2,self.weights3,gradient_output3_act)\n",
    "            gradient_output2_act                  = self.ReLU_backward(gradient_input3, self.output2)\n",
    "            gradient_output2, gradient_input2     = self.linear_backward(self.output1, self.weights2, gradient_output2_act)\n",
    "            gradient_output1_act                  = self.ReLU_backward(gradient_input2, self.output1)\n",
    "            gradient_output1, gradient_input1     = self.linear_backward(self.X, self.weights1, gradient_output1_act)\n",
    "            \n",
    "            self.weights3  = self.weights3 - learning_rate * gradient_output3\n",
    "            self.weights2  = self.weights2 - learning_rate * gradient_output2\n",
    "            self.weights1  = self.weights1 - learning_rate * gradient_output1\n",
    "            assert np.sum(gradient_output1) != np.nan, \"The gradient has nan\"\n",
    "            assert np.sum(gradient_output1) != np.inf, \"The gradient has inf\"\n",
    "            if i%100 == 0:\n",
    "\n",
    "                loss = self.compute_loss(self.output3_act, y)\n",
    "                self.accuracy = np.mean(predictions==self.y)\n",
    "                if(self.accuracy > 99.0):\n",
    "                    break\n",
    "                print(f'Loss after a iteration {i}:{loss} || Accuracy: {self.accuracy * 100}')\n",
    "                \n",
    "    def bgd_with_l2(self,X, y, learning_rate= 0.01, iteration = 10000):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.loss_list = []\n",
    "        self.acc_list = []\n",
    "        for i in range(iteration):\n",
    "            self.forward_pass(self.X)\n",
    "            predictions = np.argmax(self.output3_act, axis=1)\n",
    "            gradient_output3_act                  = self.softmax_categorical_cross_entropy_combined_backward(self.output3_act, self.y)\n",
    "#             print(gradient_output3_act)\n",
    "            gradient_output3, gradient_input3     = self.linear_backward_with_l2(self.output2,self.weights3,gradient_output3_act)\n",
    "            gradient_output2_act                  = self.ReLU_backward(gradient_input3, self.output2)\n",
    "            gradient_output2, gradient_input2     = self.linear_backward_with_l2(self.output1, self.weights2, gradient_output2_act)\n",
    "            gradient_output1_act                  = self.ReLU_backward(gradient_input2, self.output1)\n",
    "            gradient_output1, gradient_input1     = self.linear_backward_with_l2(self.X, self.weights1, gradient_output1_act)\n",
    "            \n",
    "            self.weights3  = self.weights3 - learning_rate * gradient_output3\n",
    "            self.weights2  = self.weights2 - learning_rate * gradient_output2\n",
    "            self.weights1  = self.weights1 - learning_rate * gradient_output1\n",
    "            assert np.sum(gradient_output1) != np.nan, \"The gradient has nan\"\n",
    "            assert np.sum(gradient_output1) != np.inf, \"The gradient has inf\"\n",
    "            loss = self.compute_loss_with_l2(self.output3_act, y)\n",
    "            self.accuracy = np.mean(predictions==self.y)\n",
    "            if i%100 == 0:\n",
    "                self.loss_list.append(loss)\n",
    "                self.acc_list.append(self.accuracy)\n",
    "                if(self.accuracy > .99):\n",
    "                    break\n",
    "                print(f'Loss after a iteration {i}:{loss} || Accuracy: {self.accuracy * 100}')\n",
    "        plt.plot(self.loss_list)\n",
    "        plt.title(\"Training loss of the model\")   \n",
    "    \n",
    "    def rand_mini_batches(self, X, Y, mini_batch_size = 128, seed=1):\n",
    "   \n",
    "        classes = Y.shape[0]\n",
    "        np.random.seed(seed)            \n",
    "        m = X.shape[0]                  # number of training examples\n",
    "        mini_batches = []\n",
    "\n",
    "    #     Shuffle (X, Y)\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[permutation, :]\n",
    "    #     shuffled_Y = Y[:, permutation].reshape((classes,m))\n",
    "        shuffled_Y = Y.iloc[permutation]\n",
    "\n",
    "    #     Partition (shuffled_X, shuffled_Y) except for the last batch\n",
    "        num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size \n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[ k * mini_batch_size : (k+1)*mini_batch_size, : ]\n",
    "    #         mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1)*mini_batch_size]\n",
    "            mini_batch_Y = shuffled_Y.iloc[k * mini_batch_size : (k+1)*mini_batch_size]\n",
    "\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        # Last batch (last mini-batch < mini_batch_size)\n",
    "        if m % mini_batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "    #         mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "            mini_batch_Y = shuffled_Y.iloc[ num_complete_minibatches * mini_batch_size : m]\n",
    "\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        return mini_batches   \n",
    "    \n",
    "    def mini_batch_gd(self, X, y, learning_rate= 0.01, iteration = 100, mini_batch_size = 128):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.loss_list = []\n",
    "        self.acc_list = []\n",
    "        for i in range(iteration):\n",
    "            seed = iteration\n",
    "            mini_batches= self.rand_mini_batches(X =  self.X_train, Y = self.y_train, mini_batch_size = mini_batch_size, seed=seed)\n",
    "            for mini_batch in mini_batches:\n",
    "                mini_batch_X , mini_batch_y = mini_batch\n",
    "                self.forward_pass(mini_batch_X)\n",
    "                predictions = np.argmax(self.output3_act, axis=1)\n",
    "                gradient_output3_act                  = self.softmax_categorical_cross_entropy_combined_backward(self.output3_act, mini_batch_y)\n",
    "    #             print(gradient_output3_act)\n",
    "                gradient_output3, gradient_input3     = self.linear_backward_with_l2(self.output2,self.weights3,gradient_output3_act)\n",
    "                gradient_output2_act                  = self.ReLU_backward(gradient_input3, self.output2)\n",
    "                gradient_output2, gradient_input2     = self.linear_backward_with_l2(self.output1, self.weights2, gradient_output2_act)\n",
    "                gradient_output1_act                  = self.ReLU_backward(gradient_input2, self.output1)\n",
    "                gradient_output1, gradient_input1     = self.linear_backward_with_l2(mini_batch_X, self.weights1, gradient_output1_act)\n",
    "\n",
    "                self.weights3  = self.weights3 - learning_rate * gradient_output3\n",
    "                self.weights2  = self.weights2 - learning_rate * gradient_output2\n",
    "                self.weights1  = self.weights1 - learning_rate * gradient_output1\n",
    "                assert np.sum(gradient_output1) != np.nan, \"The gradient has nan\"\n",
    "                assert np.sum(gradient_output1) != np.inf, \"The gradient has inf\"\n",
    "                loss = self.compute_loss_with_l2(self.output3_act, mini_batch_y)\n",
    "                self.accuracy = np.mean(predictions==mini_batch_y)\n",
    "            self.loss_list.append(loss)\n",
    "            self.acc_list.append(self.accuracy)\n",
    "            print(f'Loss after a epoch {i+1}:{loss} || Accuracy: {self.accuracy * 100}')\n",
    "            \n",
    "        plt.plot(self.loss_list)\n",
    "        plt.title(\"Training loss of the model\")\n",
    "        \n",
    "    def load_model(self, weights, biases = None):\n",
    "        self.weights1 = weights['1']\n",
    "        self.weights2 = weights['2']\n",
    "        self.weights3 = weights['3']\n",
    "        \n",
    "        self.biases1 = weights['b1']\n",
    "        self.biases2 = weights['b2']\n",
    "        self.biases3 = weights['b3']\n",
    "    \n",
    "    def save_model(self, filename = f'model'):\n",
    "        from datetime import date\n",
    "\n",
    "        today = date.today()\n",
    "\n",
    "        filename = f'{filename}_{self.accuracy}-{today}.pkl'\n",
    "        weights = {\n",
    "                    '1': self.weights1, '2': self.weights2, '3': self.weights3, \n",
    "                    'b1':self.biases1,'b2':self.biases2,'b3':self.biases3\n",
    "        }\n",
    "        pickle.dump(weights, open(filename, 'wb'))\n",
    "    \n",
    "    def fit(self, X,y, optimizer = 'bgd', iters = 10000, learning_rate = 0.001, batch_size = None, regularizer = 'l2'):\n",
    "        '''\n",
    "        A Method to fit the Model\n",
    "        X<pd.DataFrame> : The set of features\n",
    "        y<pd.DataFrame> : The target Labels\n",
    "        optimizer<str> : The optimizer for the model (bgd, mini_batch)\n",
    "        learning_rate<float>: The learning rate for the model\n",
    "        batch_size<int>(optional): 1 results in SGD, optional when optimizer = 'bgd' \n",
    "        regularizer<str>(optional) :values(None/'l2') The regularizer for the system \n",
    "        \n",
    "        '''\n",
    "        if (optimizer == 'bgd' and  regularizer is None) :\n",
    "            self.bgd(self, X, y, learning_rate= learning_rate, iteration = iteration)\n",
    "        elif (optimizer == 'bgd') and (regularizer == 'l2'):\n",
    "            self.bgd_with_l2(X = X, y = y, learning_rate = learning_rate, iteration = iteration)\n",
    "        elif (optimizer == 'mini_batch'):\n",
    "            if batch_size is not None:\n",
    "                self.mini_batch_gd(X = X, y= y, learning_rate= learning_rate, iteration = iters, mini_batch_size = batch_size)\n",
    "            else: \n",
    "                raise Exception(\"Specify the batch_size for Mini Batch Gradient Descent\")\n",
    "        else:\n",
    "            raise Exception(\"Invalid parameters passed !\")\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        output1     = self.forward(X_test, self.weights1, self.biases1)\n",
    "        output1_act = self.ReLU(output1)\n",
    "        output2     = self.forward(output1_act, self.weights2, self.biases2)\n",
    "        output2_act = self.ReLU(output2)\n",
    "        output3     = self.forward(output2_act, self.weights3, self.biases3)\n",
    "        output3_act = self.Softmax(output3)\n",
    "        print(output3_act)\n",
    "        prediction, prediction_prob = np.argmax(output3_act, axis=1), np.max(output3_act, axis=1)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f16fc187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after a epoch 1:2.007842838481319 || Accuracy: 0.0\n",
      "Loss after a epoch 2:0.9014077806397884 || Accuracy: 0.0\n",
      "Loss after a epoch 3:0.8914735852156634 || Accuracy: 0.0\n",
      "Loss after a epoch 4:0.8999500198066268 || Accuracy: 0.0\n",
      "Loss after a epoch 5:0.6697822820318858 || Accuracy: 100.0\n",
      "Loss after a epoch 6:0.49706096771597086 || Accuracy: 100.0\n",
      "Loss after a epoch 7:0.4215959020082974 || Accuracy: 100.0\n",
      "Loss after a epoch 8:0.29011541477562747 || Accuracy: 100.0\n",
      "Loss after a epoch 9:0.2512372946196863 || Accuracy: 100.0\n",
      "Loss after a epoch 10:0.14467552017925275 || Accuracy: 100.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model  = Classifier(1024, [32,32, 10])\n",
    "model.fit(X_train, y_train, optimizer = 'mini_batch' , iters = 10, batch_size = 1, learning_rate = 0.00042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45ebb227",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"SGD_Model_EPOCH_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7c876e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.36868033e-04 4.18117970e-03 6.33534730e-01 ... 4.66523451e-04\n",
      "  1.54580965e-02 2.24047830e-03]\n",
      " [9.80783869e-01 4.05217344e-03 2.21596021e-04 ... 8.31021422e-03\n",
      "  3.00986820e-04 5.67636947e-03]\n",
      " [8.33569199e-05 9.88594215e-01 1.11025203e-05 ... 4.14226010e-06\n",
      "  2.16226292e-07 9.90393928e-03]\n",
      " ...\n",
      " [9.35646806e-01 2.59401208e-02 1.94793838e-02 ... 8.57479947e-03\n",
      "  6.57998996e-04 3.54610191e-03]\n",
      " [2.91920494e-04 2.90814144e-05 7.76911978e-04 ... 7.36604991e-03\n",
      "  3.53894386e-04 3.68095256e-06]\n",
      " [6.92804027e-05 1.42295240e-03 7.19843204e-04 ... 1.94966075e-04\n",
      "  2.14814808e-04 1.82531917e-04]]\n"
     ]
    }
   ],
   "source": [
    "pr = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e95dcd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20569    2\n",
       "6186     0\n",
       "11417    1\n",
       "7064     0\n",
       "59855    7\n",
       "        ..\n",
       "21211    2\n",
       "27344    3\n",
       "7100     0\n",
       "55159    6\n",
       "52081    6\n",
       "Name: label, Length: 17000, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b8f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b5557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df16d48a",
   "metadata": {},
   "source": [
    "# Check Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e4d76b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score,confusion_matrix, ConfusionMatrixDisplay\n",
    "acc = accuracy_score(y_test.values, pr)\n",
    "rec = recall_score(y_test.values, pr, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1023aade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1649,    0,    0,    0,    1,    0,    1,   11,    7,    5],\n",
       "       [   2, 1682,    3,    2,    2,    3,    4,    0,    0,    5],\n",
       "       [   0,    9, 1472,   85,    4,   24,   22,    2,    6,    3],\n",
       "       [   0,    6,  103, 1575,    3,   25,   18,    4,    0,    2],\n",
       "       [   0,    9,    1,    2, 1705,   18,    3,    4,   10,    8],\n",
       "       [   0,    2,   27,   16,   26, 1590,    9,    5,    2,    1],\n",
       "       [   7,   11,    5,   18,    9,    7, 1583,   28,    8,   18],\n",
       "       [  24,    0,    2,    4,    2,   15,   27, 1661,    2,    0],\n",
       "       [  11,    0,    2,    0,    2,    1,   14,    1, 1624,   11],\n",
       "       [   2,   17,    2,    3,    5,    0,   13,    3,    7, 1673]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_test.values, pr )\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "407271c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.953639785345603"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99727d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
