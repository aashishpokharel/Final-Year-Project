{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d63ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd65f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = {\n",
    "                'digit_0' : 0,\n",
    "                'digit_1' : 1,\n",
    "                'digit_2' : 2,\n",
    "                'digit_3' : 3,\n",
    "                'digit_4' : 4,\n",
    "                'digit_5' : 5,\n",
    "                'digit_6' : 6,\n",
    "                'digit_7' : 7,\n",
    "                'digit_8' : 8,\n",
    "                'digit_9' : 9,\n",
    "}\n",
    "train_data = pd.read_csv('./dataset/digit_all_augmented.csv')\n",
    "test_data  = pd.read_csv('./dataset/test_digits_data.csv')\n",
    "X_train = train_data.iloc[:, 1:-1].values\n",
    "y_train = train_data.iloc[:, -1]\n",
    "y_train = y_train.replace(charset)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61885a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "mm = StandardScaler()\n",
    "X_train = mm.fit_transform(X_train)\n",
    "# X_dev   = mm.fit_transform(X_dev)\n",
    "X_test = mm.transform(X_test)\n",
    "pickle.dump(mm,open('scaler_norm.pkl', 'wb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f76d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Before Processing\t After Processing\n",
      "=================================================================\n",
      "Training Set Images:\t(85000, 1026)\t\t(68000, 1024)\n",
      "Training Set Labels:\t(85000,)\t\t(68000,)\n",
      "Test Set Images:\t(3000, 1025)\t\t(17000, 1024)\n",
      "Test Set Labels:\t(3000,)\t\t\t(3000,)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Data\\t\\t\\t\",\"Before Processing\\t\",\"After Processing\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(train_data.shape)+\"\\t\\t\"+ str(X_train.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(train_data.iloc[:, -1].shape)+\"\\t\\t\"+ str(y_train.shape))\n",
    "# print(\"Dev Set Images:\\t\\t\" + str(X_dev.shape)+\"\\t\\t\"+ str(X_dev.shape))\n",
    "# print(\"Dev Set Labels:\\t\\t\" + str(y_dev.shape)+\"\\t\\t\\t\"+ str(y_dev.shape))\n",
    "print(\"Test Set Images:\\t\" + str(test_data.shape)+\"\\t\\t\"+ str(X_test.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(test_data.iloc[:, -1].shape)+\"\\t\\t\\t\"+ str(test_data.iloc[:, -1].shape))\n",
    "print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a695ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier:\n",
    "    def __init__(self, n_inputs, n_neurons = [32,32,10], initialization = 'random', seed = 42):\n",
    "        np.random.seed(seed)\n",
    "        # We have done here n_inputs/n_neurons instead of n_neurons/n_inputs to prevent the Transpose everytime\n",
    "        if initialization == 'random':\n",
    "            self.weights1   = 0.01 * np.random.randn(n_inputs, n_neurons[0]) # The input shape and no of neurons you want to have in the layer\n",
    "            self.biases1    =  0.01 * np.random.randn(1, n_neurons[0])\n",
    "            self.weights2   = 0.01 *np.random.randn(n_neurons[0], n_neurons[1]) # The input shape and no of neurons you want to have in the layer\n",
    "            self.biases2    = 0.01 * np.random.randn(1, n_neurons[1])\n",
    "            self.weights3   = 0.01 * np.random.randn(n_neurons[1], n_neurons[2])\n",
    "            self.biases3    = 0.01 * np.random.randn(1, n_neurons[2])\n",
    "        if initialization == 'He':\n",
    "            self.weights1   =  np.sqrt(2/n_neurons[0]) * np.random.randn(n_inputs, n_neurons[0]) # The input shape and no of neurons you want to have in the layer\n",
    "            self.biases1    =  np.sqrt(2/n_neurons[0]) * np.random.randn(1, n_neurons[0])\n",
    "            self.weights2   =  np.sqrt(2/n_neurons[1]) *np.random.randn(n_neurons[0], n_neurons[1]) # The input shape and no of neurons you want to have in the layer\n",
    "            self.biases2    =  np.sqrt(2/n_neurons[1]) * np.random.randn(1, n_neurons[1])\n",
    "            self.weights3   =  np.sqrt(2/n_neurons[2]) * np.random.randn(n_neurons[1], n_neurons[2])\n",
    "            self.biases3    =  np.sqrt(2/n_neurons[2]) * np.random.randn(1, n_neurons[2])\n",
    "        self.output1 = None\n",
    "        self.output2 = None\n",
    "        self.output3 = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        # activatied outputs\n",
    "        self.output1_act = None\n",
    "        self.output2_act = None\n",
    "        self.output3_act = None\n",
    "    def forward(self, inputs, weights, biases):\n",
    "        \"\"\" The dot product of the input - weights - Biases (y = Wx + b) \"\"\"\n",
    "        output = np.dot(inputs, weights) + biases\n",
    "        if(np.isnan(np.sum(output))):\n",
    "            raise Exception(\"NaN values present in FW pass\")\n",
    "        elif(np.isinf(np.sum(output))):\n",
    "            raise Exception(\"INF values present in FW Pass\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def ReLU(self, inputs):\n",
    "        \"\"\" Rectified Linear Activation Function \"\"\"\n",
    "        output = np.maximum(0, inputs)\n",
    "        return output\n",
    "    \n",
    "    def Softmax(self, inputs):\n",
    "    # subtract largest value to prevent overflow\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        if(np.isnan(np.sum(probabilities))):\n",
    "            raise Exception(\"NaN values present in Softmax For\")\n",
    "        elif(np.isinf(np.sum(probabilities))):\n",
    "            raise Exception(\"INF values present in Softmax For\")\n",
    "        \n",
    "        return probabilities\n",
    "        \n",
    "    def categorical_cross_entropy(self,y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-6, 1-1e-6)\n",
    "        # Handling if labels are 1D \n",
    "        correct_confidences = None\n",
    "        if len(y_true.shape) == 1:\n",
    "#             print(y_pred_clipped[range(samples), :].shape)\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) ==2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis =1)\n",
    "        else:\n",
    "            raise Exception(\"Sorry, no numbers below zero\")\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "#         print(negative_log_likelihoods.shape)\n",
    "        return negative_log_likelihoods \n",
    "    \n",
    "    def linear_backward(self,inputs, weights, dvalues):\n",
    "        self.dweights_linear = np.dot(inputs.T, dvalues)\n",
    "        self.dbiases_linear = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinput_linear = np.dot(dvalues, weights.T)\n",
    "        \n",
    "        if(np.isnan(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"NaN values present in Linear Back\")\n",
    "        elif(np.isinf(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"INF values present in Linear BAck\")\n",
    "        \n",
    "        \n",
    "        return self.dweights_linear, self.dinput_linear\n",
    "    \n",
    "    def linear_backward_with_l2(self,inputs, weights, dvalues, lambd = 0.5):\n",
    "        \"\"\"  \"\"\"\n",
    "        m = inputs.shape[1]\n",
    "        self.dweights_linear = np.dot(inputs.T, dvalues) + (lambd*weights)/m\n",
    "        self.dbiases_linear = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinput_linear = np.dot(dvalues, weights.T) \n",
    "        \n",
    "        if(np.isnan(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"NaN values present in Linear Back\")\n",
    "        elif(np.isinf(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"INF values present in Linear BAck\")\n",
    "        \n",
    "        \n",
    "        return self.dweights_linear, self.dinput_linear\n",
    "    \n",
    "    def softmax_backward(self,dA, Z):\n",
    "        \"\"\"Compute backward pass for softmax activation\"\"\"\n",
    "        softmax_output = self.Softmax(Z) \n",
    "        return softmax_output * (1 - softmax_output) * dA\n",
    "\n",
    "    def ReLU_backward(self,dA, Z):\n",
    "        \n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        if(np.isnan(np.sum(dZ))):\n",
    "            raise Exception(\"NaN values present in RELU Back\")\n",
    "        elif(np.isinf(np.sum(dZ))):\n",
    "            raise Exception(\"INF values present in RELU BAck\")\n",
    "        return dZ\n",
    "        \n",
    "    def categorical_cross_entropy_backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs_loss = self.dinputs / samples\n",
    "        if(np.isnan(np.sum(self.dinputs))):\n",
    "            raise Exception(\"NaN values present in Softmax Back\")\n",
    "        elif(np.isinf(np.sum(self.dinputs))):\n",
    "            raise Exception(\"INF values present in Softmax_back\")\n",
    "        return self.dinputs\n",
    "    \n",
    "    def softmax_categorical_cross_entropy_combined_backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        #handling Ohe values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs_combined = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs_combined[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs_combined = self.dinputs_combined / samples\n",
    "        if(np.isnan(np.sum(self.dinputs_combined))):\n",
    "            raise Exception(\"NaN values present in Softmax Back\")\n",
    "        elif(np.isinf(np.sum(self.dinputs_combined))):\n",
    "            raise Exception(\"INF values present in Softmax_back\")\n",
    "       \n",
    "        return self.dinputs_combined\n",
    "        \n",
    "    \n",
    "    def compute_loss(self,y_pred, y_true):\n",
    "        sample_losses = self.categorical_cross_entropy(y_pred, y_true)\n",
    "        loss = np.mean(sample_losses)\n",
    "        return loss\n",
    "    \n",
    "    def compute_loss_with_l2(self,y_pred, y_true, lambd = 0.5):\n",
    "        m = 10\n",
    "        sample_losses = self.categorical_cross_entropy(y_pred, y_true)\n",
    "        L2_regularization_cost = (lambd/(2*m))*(np.sum(np.square(self.weights1) + np.sum(np.square(self.weights2) + np.sum(np.square(self.weights3)))))\n",
    "        loss = np.mean(sample_losses) \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        self.X = X\n",
    "        self.output1     = self.forward(self.X, self.weights1, self.biases1)\n",
    "        self.output1_act = self.ReLU(self.output1)\n",
    "        self.output2     = self.forward(self.output1_act, self.weights2, self.biases2)\n",
    "        self.output2_act = self.ReLU(self.output2)\n",
    "        self.output3     = self.forward(self.output2_act, self.weights3, self.biases3)\n",
    "        self.output3_act = self.Softmax(self.output3)\n",
    "#         print(\"Softmax SUM\", np.sum(self.output3_act, axis = 1))\n",
    "        if(np.isnan(np.sum(self.output3_act))):\n",
    "            raise Exception(\"NaN values present in data\")\n",
    "        elif(np.isinf(np.sum(self.output3_act))):\n",
    "            raise Exception(\"INF values present in data\")\n",
    "        \n",
    "        \n",
    "    def check_inf(self):\n",
    "        check_weights = np.any(np.isinf(self.weights1)) or np.any(np.isinf(self.weights2)) or np.any(np.isinf(self.weights3))\n",
    "        check_bias    = np.any(np.isinf(self.biases1)) or np.any(np.isinf(self.biases2)) or np.any(np.isinf(self.biases3))\n",
    "        return (check_weights or check_bias)\n",
    "    \n",
    "    def bgd(self, X, y, learning_rate= 0.1, iteration = 10000):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        for i in range(iteration):\n",
    "            self.forward_pass(self.X)\n",
    "            predictions = np.argmax(self.output3_act, axis=1)\n",
    "            \n",
    "            \n",
    "            gradient_output3_act                  = self.softmax_categorical_cross_entropy_combined_backward(self.output3_act, self.y)\n",
    "            gradient_output3, gradient_input3     = self.linear_backward(self.output2,self.weights3,gradient_output3_act)\n",
    "            gradient_output2_act                  = self.ReLU_backward(gradient_input3, self.output2)\n",
    "            gradient_output2, gradient_input2     = self.linear_backward(self.output1, self.weights2, gradient_output2_act)\n",
    "            gradient_output1_act                  = self.ReLU_backward(gradient_input2, self.output1)\n",
    "            gradient_output1, gradient_input1     = self.linear_backward(self.X, self.weights1, gradient_output1_act)\n",
    "            \n",
    "            self.weights3  = self.weights3 - learning_rate * gradient_output3\n",
    "            self.weights2  = self.weights2 - learning_rate * gradient_output2\n",
    "            self.weights1  = self.weights1 - learning_rate * gradient_output1\n",
    "            assert np.sum(gradient_output1) != np.nan, \"The gradient has nan\"\n",
    "            assert np.sum(gradient_output1) != np.inf, \"The gradient has inf\"\n",
    "            if i%100 == 0:\n",
    "\n",
    "                loss = self.compute_loss(self.output3_act, y)\n",
    "                self.accuracy = np.mean(predictions==self.y)\n",
    "                if(self.accuracy > 99.0):\n",
    "                    break\n",
    "                print(f' Training Loss after a iteration {i}:{loss} || Accuracy: {self.accuracy * 100}')\n",
    "                \n",
    "    def bgd_with_l2(self,X, y, learning_rate= 0.01, iteration = 10000):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.loss_list = []\n",
    "        self.acc_list = []\n",
    "        for i in range(iteration):\n",
    "            self.forward_pass(self.X)\n",
    "            predictions = np.argmax(self.output3_act, axis=1)\n",
    "            gradient_output3_act                  = self.softmax_categorical_cross_entropy_combined_backward(self.output3_act, self.y)\n",
    "#             print(gradient_output3_act)\n",
    "            gradient_output3, gradient_input3     = self.linear_backward_with_l2(self.output2,self.weights3,gradient_output3_act)\n",
    "            gradient_output2_act                  = self.ReLU_backward(gradient_input3, self.output2)\n",
    "            gradient_output2, gradient_input2     = self.linear_backward_with_l2(self.output1, self.weights2, gradient_output2_act)\n",
    "            gradient_output1_act                  = self.ReLU_backward(gradient_input2, self.output1)\n",
    "            gradient_output1, gradient_input1     = self.linear_backward_with_l2(self.X, self.weights1, gradient_output1_act)\n",
    "            \n",
    "            self.weights3  = self.weights3 - learning_rate * gradient_output3\n",
    "            self.weights2  = self.weights2 - learning_rate * gradient_output2\n",
    "            self.weights1  = self.weights1 - learning_rate * gradient_output1\n",
    "            assert np.sum(gradient_output1) != np.nan, \"The gradient has nan\"\n",
    "            assert np.sum(gradient_output1) != np.inf, \"The gradient has inf\"\n",
    "            loss = self.compute_loss_with_l2(self.output3_act, y)\n",
    "            self.accuracy = np.mean(predictions==self.y)\n",
    "            if i%100 == 0:\n",
    "                self.loss_list.append(loss)\n",
    "                self.acc_list.append(self.accuracy)\n",
    "                if(self.accuracy > .99):\n",
    "                    break\n",
    "                print(f'Loss after a iteration {i}:{loss} || Accuracy: {self.accuracy * 100}')\n",
    "        plt.plot(self.loss_list)\n",
    "        plt.title(\"Training loss of the model\")   \n",
    "    \n",
    "    def rand_mini_batches(self, X, Y, mini_batch_size = 128, seed=1):\n",
    "   \n",
    "        classes = Y.shape[0]\n",
    "        np.random.seed(seed)            \n",
    "        m = X.shape[0]                  # number of training examples\n",
    "        mini_batches = []\n",
    "\n",
    "    #     Shuffle (X, Y)\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[permutation, :]\n",
    "    #     shuffled_Y = Y[:, permutation].reshape((classes,m))\n",
    "        shuffled_Y = Y.iloc[permutation]\n",
    "\n",
    "    #     Partition (shuffled_X, shuffled_Y) except for the last batch\n",
    "        num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size \n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[ k * mini_batch_size : (k+1)*mini_batch_size, : ]\n",
    "    #         mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1)*mini_batch_size]\n",
    "            mini_batch_Y = shuffled_Y.iloc[k * mini_batch_size : (k+1)*mini_batch_size]\n",
    "\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        # Last batch (last mini-batch < mini_batch_size)\n",
    "        if m % mini_batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "    #         mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "            mini_batch_Y = shuffled_Y.iloc[ num_complete_minibatches * mini_batch_size : m]\n",
    "\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        return mini_batches   \n",
    "    \n",
    "    def mini_batch_gd(self, X, y, learning_rate= 0.01, iteration = 100, mini_batch_size = 128):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.loss_list = []\n",
    "        self.acc_list = []\n",
    "        for i in range(iteration):\n",
    "            seed = iteration\n",
    "            mini_batches= self.rand_mini_batches(X =  self.X_train, Y = self.y_train, mini_batch_size = mini_batch_size, seed=seed)\n",
    "            for mini_batch in mini_batches:\n",
    "                mini_batch_X , mini_batch_y = mini_batch\n",
    "                self.forward_pass(mini_batch_X)\n",
    "                predictions = np.argmax(self.output3_act, axis=1)\n",
    "                gradient_output3_act                  = self.softmax_categorical_cross_entropy_combined_backward(self.output3_act, mini_batch_y)\n",
    "    #             print(gradient_output3_act)\n",
    "                gradient_output3, gradient_input3     = self.linear_backward_with_l2(self.output2,self.weights3,gradient_output3_act)\n",
    "                gradient_output2_act                  = self.ReLU_backward(gradient_input3, self.output2)\n",
    "                gradient_output2, gradient_input2     = self.linear_backward_with_l2(self.output1, self.weights2, gradient_output2_act)\n",
    "                gradient_output1_act                  = self.ReLU_backward(gradient_input2, self.output1)\n",
    "                gradient_output1, gradient_input1     = self.linear_backward_with_l2(mini_batch_X, self.weights1, gradient_output1_act)\n",
    "\n",
    "                self.weights3  = self.weights3 - learning_rate * gradient_output3\n",
    "                self.weights2  = self.weights2 - learning_rate * gradient_output2\n",
    "                self.weights1  = self.weights1 - learning_rate * gradient_output1\n",
    "                assert np.sum(gradient_output1) != np.nan, \"The gradient has nan\"\n",
    "                assert np.sum(gradient_output1) != np.inf, \"The gradient has inf\"\n",
    "                loss = self.compute_loss_with_l2(self.output3_act, mini_batch_y)\n",
    "                self.accuracy = np.mean(predictions==mini_batch_y)\n",
    "                predictions_test = self.predict(self.X_val)\n",
    "                test_accuracy = np.mean(predictions_test == self.y_val)\n",
    "            self.loss_list.append(loss)\n",
    "            self.acc_list.append(self.accuracy)\n",
    "            print(f'Epoch({i+1}): Training Loss :{\"%.2f\"%loss} || Training Accuracy: {\"%.2f\"%(self.accuracy * 100)}%|| Validation Accuracy :{\"%.2f\"%(test_accuracy * 100)}% ')\n",
    "        plt.plot(self.loss_list)\n",
    "        plt.title(\"Training loss of the model\")\n",
    "        \n",
    "    def load_model(self, weights, biases = None):\n",
    "        self.weights1 = weights['1']\n",
    "        self.weights2 = weights['2']\n",
    "        self.weights3 = weights['3']\n",
    "        \n",
    "        self.biases1 = weights['b1']\n",
    "        self.biases2 = weights['b2']\n",
    "        self.biases3 = weights['b3']\n",
    "    \n",
    "    def save_model(self, filename = f'model'):\n",
    "        from datetime import date\n",
    "\n",
    "        today = date.today()\n",
    "\n",
    "        filename = f'{filename}_{self.accuracy}-{today}.pkl'\n",
    "        weights = {\n",
    "                    '1': self.weights1, '2': self.weights2, '3': self.weights3, \n",
    "                    'b1':self.biases1,'b2':self.biases2,'b3':self.biases3\n",
    "        }\n",
    "        pickle.dump(weights, open(filename, 'wb'))\n",
    "    \n",
    "    def fit(self, X,y, optimizer = 'bgd', iters = 10000, learning_rate = 0.001, batch_size = None, regularizer = 'l2'):\n",
    "        '''\n",
    "        A Method to fit the Model\n",
    "        X<pd.DataFrame> : The set of features\n",
    "        y<pd.DataFrame> : The target Labels\n",
    "        optimizer<str> : The optimizer for the model (bgd, mini_batch)\n",
    "        learning_rate<float>: The learning rate for the model\n",
    "        batch_size<int>(optional): 1 results in SGD, optional when optimizer = 'bgd' \n",
    "        regularizer<str>(optional) :values(None/'l2') The regularizer for the system \n",
    "        \n",
    "        '''\n",
    "        if (optimizer == 'bgd' and  regularizer is None) :\n",
    "            self.bgd(self, X, y, learning_rate= learning_rate, iteration = iteration)\n",
    "        elif (optimizer == 'bgd') and (regularizer == 'l2'):\n",
    "            self.bgd_with_l2(X = X, y = y, learning_rate = learning_rate, iteration = iteration)\n",
    "        elif (optimizer == 'mini_batch'):\n",
    "            if batch_size is not None:\n",
    "                self.mini_batch_gd(X = X, y= y, learning_rate= learning_rate, iteration = iters, mini_batch_size = batch_size)\n",
    "            else: \n",
    "                raise Exception(\"Specify the batch_size for Mini Batch Gradient Descent\")\n",
    "        else:\n",
    "            raise Exception(\"Invalid parameters passed !\")\n",
    "            \n",
    "    def get_validation_data(self, X_val, y_val):\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        wt1, wt2, wt3 = self.weights1.copy(), self.weights2.copy(), self.weights3.copy()\n",
    "        b1, b2, b3    = self.biases1.copy(), self.biases2.copy(), self.biases3.copy()\n",
    "        output1     = self.forward(X_test, wt1, b1)\n",
    "        output1_act = self.ReLU(output1)\n",
    "        output2     = self.forward(output1_act,  wt2, b2)\n",
    "        output2_act = self.ReLU(output2)\n",
    "        output3     = self.forward(output2_act,  wt3, b3)\n",
    "        output3_act = self.Softmax(output3)\n",
    "        print(output3_act)\n",

    "        prediction, prediction_prob = np.argmax(output3_act, axis=1), np.max(output3_act, axis=1)\n",
    "        return prediction\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        wt1, wt2, wt3 = self.weights1.copy(), self.weights2.copy(), self.weights3.copy()\n",
    "        b1, b2, b3    = self.biases1.copy(), self.biases2.copy(), self.biases3.copy()\n",
    "        output1       = self.forward(X_test, wt1, b1)\n",
    "        output1_act   = self.ReLU(output1)\n",
    "        output2       = self.forward(output1_act,  wt2, b2)\n",
    "        output2_act   = self.ReLU(output2)\n",
    "        output3       = self.forward(output2_act,  wt3, b3)\n",
    "        output3_act   = self.Softmax(output3)\n",
    "        prediction, prediction_prob = np.argmax(output3_act, axis=1), np.max(output3_act, axis=1)\n",
    "        output3_selected = output3_act\n",
    "        values = []\n",
    "        for i in range(0,3):\n",
    "            prediction_i, prediction_prob_i= np.argmax(output3_selected, axis=1), np.max(output3_selected, axis=1)\n",
    "            output3_selected[0][prediction_i] = -1\n",
    "            values.append([prediction_i[0], prediction_prob_i[0]])\n",
    "        return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f16fc187",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch(1): Training Loss :0.87 || Training Accuracy: 68.75%|| Validation Accuracy :64.56% \n",
      "Epoch(2): Training Loss :0.67 || Training Accuracy: 68.75%|| Validation Accuracy :71.04% \n",
      "Epoch(3): Training Loss :0.54 || Training Accuracy: 81.25%|| Validation Accuracy :74.81% \n",
      "Epoch(4): Training Loss :0.47 || Training Accuracy: 84.38%|| Validation Accuracy :76.76% \n",
      "Epoch(5): Training Loss :0.38 || Training Accuracy: 90.62%|| Validation Accuracy :78.49% \n",
      "Epoch(6): Training Loss :0.34 || Training Accuracy: 90.62%|| Validation Accuracy :79.71% \n",
      "Epoch(7): Training Loss :0.29 || Training Accuracy: 90.62%|| Validation Accuracy :80.99% \n",
      "Epoch(8): Training Loss :0.27 || Training Accuracy: 93.75%|| Validation Accuracy :81.97% \n",
      "Epoch(9): Training Loss :0.25 || Training Accuracy: 93.75%|| Validation Accuracy :82.65% \n",
      "Epoch(10): Training Loss :0.24 || Training Accuracy: 93.75%|| Validation Accuracy :83.48% \n",
      "Epoch(11): Training Loss :0.24 || Training Accuracy: 93.75%|| Validation Accuracy :83.98% \n",
      "Epoch(12): Training Loss :0.24 || Training Accuracy: 93.75%|| Validation Accuracy :84.32% \n",
      "Epoch(13): Training Loss :0.25 || Training Accuracy: 90.62%|| Validation Accuracy :84.75% \n",
      "Epoch(14): Training Loss :0.25 || Training Accuracy: 90.62%|| Validation Accuracy :85.30% \n",
      "Epoch(15): Training Loss :0.24 || Training Accuracy: 93.75%|| Validation Accuracy :85.77% \n",
      "Epoch(16): Training Loss :0.24 || Training Accuracy: 93.75%|| Validation Accuracy :86.05% \n",
      "Epoch(17): Training Loss :0.24 || Training Accuracy: 93.75%|| Validation Accuracy :86.21% \n",
      "Epoch(18): Training Loss :0.24 || Training Accuracy: 93.75%|| Validation Accuracy :86.48% \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model  = Classifier(1024, [32,32, 10])\n",
    "model.forward_pass(X_test)\n",
    "loss = model.compute_loss_with_l2(model.output3_act, y_test)\n",
    "model.backward_pass_with_l2(y_test, iteration = 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ebb227",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"SGD_Model_EPOCH_3\")"
   ]
  },
  {
   "cell_type": "code",
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e95dcd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20569    2\n",
       "6186     0\n",
       "11417    1\n",
       "7064     0\n",
       "59855    7\n",
       "        ..\n",
       "21211    2\n",
       "27344    3\n",
       "7100     0\n",
       "55159    6\n",
       "52081    6\n",
       "Name: label, Length: 17000, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b8f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b5557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df16d48a",
   "metadata": {},
   "source": [
    "# Check Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e4d76b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score,confusion_matrix, ConfusionMatrixDisplay\n",
    "acc = accuracy_score(y_test.values, pr)\n",
    "rec = recall_score(y_test.values, pr, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1023aade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1649,    0,    0,    0,    1,    0,    1,   11,    7,    5],\n",
       "       [   2, 1682,    3,    2,    2,    3,    4,    0,    0,    5],\n",
       "       [   0,    9, 1472,   85,    4,   24,   22,    2,    6,    3],\n",
       "       [   0,    6,  103, 1575,    3,   25,   18,    4,    0,    2],\n",
       "       [   0,    9,    1,    2, 1705,   18,    3,    4,   10,    8],\n",
       "       [   0,    2,   27,   16,   26, 1590,    9,    5,    2,    1],\n",
       "       [   7,   11,    5,   18,    9,    7, 1583,   28,    8,   18],\n",
       "       [  24,    0,    2,    4,    2,   15,   27, 1661,    2,    0],\n",
       "       [  11,    0,    2,    0,    2,    1,   14,    1, 1624,   11],\n",
       "       [   2,   17,    2,    3,    5,    0,   13,    3,    7, 1673]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_test.values, pr )\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "407271c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.953639785345603"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99727d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
