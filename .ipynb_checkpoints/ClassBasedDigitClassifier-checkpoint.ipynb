{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d63ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd65f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = {\n",
    "                'digit_0' : 0,\n",
    "                'digit_1' : 1,\n",
    "                'digit_2' : 2,\n",
    "                'digit_3' : 3,\n",
    "                'digit_4' : 4,\n",
    "                'digit_5' : 5,\n",
    "                'digit_6' : 6,\n",
    "                'digit_7' : 7,\n",
    "                'digit_8' : 8,\n",
    "                'digit_9' : 9,\n",
    "}\n",
    "train_data = pd.read_csv('./dataset/digit_all_augmented.csv')\n",
    "test_data  = pd.read_csv('./dataset/test_digits_data.csv')\n",
    "X_train = train_data.iloc[:, 1:-1].values\n",
    "y_train = train_data.iloc[:, -1]\n",
    "y_train = y_train.replace(charset)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61885a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "mm = StandardScaler()\n",
    "X_train = mm.fit_transform(X_train)\n",
    "# X_dev   = mm.fit_transform(X_dev)\n",
    "X_test = mm.transform(X_test)\n",
    "pickle.dump(mm,open('scaler_norm.pkl', 'wb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f76d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Before Processing\t After Processing\n",
      "=================================================================\n",
      "Training Set Images:\t(85000, 1026)\t\t(68000, 1024)\n",
      "Training Set Labels:\t(85000,)\t\t(68000,)\n",
      "Test Set Images:\t(3000, 1025)\t\t(17000, 1024)\n",
      "Test Set Labels:\t(3000,)\t\t\t(3000,)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Data\\t\\t\\t\",\"Before Processing\\t\",\"After Processing\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(train_data.shape)+\"\\t\\t\"+ str(X_train.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(train_data.iloc[:, -1].shape)+\"\\t\\t\"+ str(y_train.shape))\n",
    "# print(\"Dev Set Images:\\t\\t\" + str(X_dev.shape)+\"\\t\\t\"+ str(X_dev.shape))\n",
    "# print(\"Dev Set Labels:\\t\\t\" + str(y_dev.shape)+\"\\t\\t\\t\"+ str(y_dev.shape))\n",
    "print(\"Test Set Images:\\t\" + str(test_data.shape)+\"\\t\\t\"+ str(X_test.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(test_data.iloc[:, -1].shape)+\"\\t\\t\\t\"+ str(test_data.iloc[:, -1].shape))\n",
    "print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a695ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier:\n",
    "    def __init__(self, n_inputs, n_neurons = [32,32,10], initialization = 'random', seed = 42):\n",
    "        np.random.seed(seed)\n",
    "        # We have done here n_inputs/n_neurons instead of n_neurons/n_inputs to prevent the Transpose everytime\n",
    "        if initialization == 'random':\n",
    "            self.weights1   = 0.01 * np.random.randn(n_inputs, n_neurons[0]) # The input shape and no of neurons you want to have in the layer\n",
    "            self.biases1    =  0.01 * np.random.randn(1, n_neurons[0])\n",
    "            self.weights2   = 0.01 *np.random.randn(n_neurons[0], n_neurons[1]) # The input shape and no of neurons you want to have in the layer\n",
    "            self.biases2    = 0.01 * np.random.randn(1, n_neurons[1])\n",
    "            self.weights3   = 0.01 * np.random.randn(n_neurons[1], n_neurons[2])\n",
    "            self.biases3    = 0.01 * np.random.randn(1, n_neurons[2])\n",
    "        if initialization == 'He':\n",
    "            self.weights1   =  np.sqrt(2/n_neurons[0]) * np.random.randn(n_inputs, n_neurons[0]) # The input shape and no of neurons you want to have in the layer\n",
    "            self.biases1    =  np.sqrt(2/n_neurons[0]) * np.random.randn(1, n_neurons[0])\n",
    "            self.weights2   =  np.sqrt(2/n_neurons[1]) *np.random.randn(n_neurons[0], n_neurons[1]) # The input shape and no of neurons you want to have in the layer\n",
    "            self.biases2    =  np.sqrt(2/n_neurons[1]) * np.random.randn(1, n_neurons[1])\n",
    "            self.weights3   =  np.sqrt(2/n_neurons[2]) * np.random.randn(n_neurons[1], n_neurons[2])\n",
    "            self.biases3    =  np.sqrt(2/n_neurons[2]) * np.random.randn(1, n_neurons[2])\n",
    "        self.output1 = None\n",
    "        self.output2 = None\n",
    "        self.output3 = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        # activatied outputs\n",
    "        self.output1_act = None\n",
    "        self.output2_act = None\n",
    "        self.output3_act = None\n",
    "    def forward(self, inputs, weights, biases):\n",
    "        \"\"\" The dot product of the input - weights - Biases (y = Wx + b) \"\"\"\n",
    "        output = np.dot(inputs, weights) + biases\n",
    "        if(np.isnan(np.sum(output))):\n",
    "            raise Exception(\"NaN values present in FW pass\")\n",
    "        elif(np.isinf(np.sum(output))):\n",
    "            raise Exception(\"INF values present in FW Pass\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def ReLU(self, inputs):\n",
    "        \"\"\" Rectified Linear Activation Function \"\"\"\n",
    "        output = np.maximum(0, inputs)\n",
    "        return output\n",
    "    \n",
    "    def Softmax(self, inputs):\n",
    "    # subtract largest value to prevent overflow\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        if(np.isnan(np.sum(probabilities))):\n",
    "            raise Exception(\"NaN values present in Softmax For\")\n",
    "        elif(np.isinf(np.sum(probabilities))):\n",
    "            raise Exception(\"INF values present in Softmax For\")\n",
    "        \n",
    "        return probabilities\n",
    "        \n",
    "    def categorical_cross_entropy(self,y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-6, 1-1e-6)\n",
    "        # Handling if labels are 1D \n",
    "        correct_confidences = None\n",
    "        if len(y_true.shape) == 1:\n",
    "#             print(y_pred_clipped[range(samples), :].shape)\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) ==2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis =1)\n",
    "        else:\n",
    "            raise Exception(\"Sorry, no numbers below zero\")\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "#         print(negative_log_likelihoods.shape)\n",
    "        return negative_log_likelihoods \n",
    "    \n",
    "    def linear_backward(self,inputs, weights, dvalues):\n",
    "        self.dweights_linear = np.dot(inputs.T, dvalues)\n",
    "        self.dbiases_linear = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinput_linear = np.dot(dvalues, weights.T)\n",
    "        \n",
    "        if(np.isnan(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"NaN values present in Linear Back\")\n",
    "        elif(np.isinf(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"INF values present in Linear BAck\")\n",
    "        \n",
    "        \n",
    "        return self.dweights_linear, self.dinput_linear\n",
    "    \n",
    "    def linear_backward_with_l2(self,inputs, weights, dvalues, lambd = 0.5):\n",
    "        \"\"\"  \"\"\"\n",
    "        m = inputs.shape[1]\n",
    "        self.dweights_linear = np.dot(inputs.T, dvalues) + (lambd*weights)/m\n",
    "        self.dbiases_linear = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinput_linear = np.dot(dvalues, weights.T) \n",
    "        \n",
    "        if(np.isnan(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"NaN values present in Linear Back\")\n",
    "        elif(np.isinf(np.sum(self.dweights_linear))):\n",
    "            raise Exception(\"INF values present in Linear BAck\")\n",
    "        \n",
    "        \n",
    "        return self.dweights_linear, self.dinput_linear\n",
    "    \n",
    "    def softmax_backward(self,dA, Z):\n",
    "        \"\"\"Compute backward pass for softmax activation\"\"\"\n",
    "        softmax_output = self.Softmax(Z) \n",
    "        return softmax_output * (1 - softmax_output) * dA\n",
    "\n",
    "    def ReLU_backward(self,dA, Z):\n",
    "        \n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        if(np.isnan(np.sum(dZ))):\n",
    "            raise Exception(\"NaN values present in RELU Back\")\n",
    "        elif(np.isinf(np.sum(dZ))):\n",
    "            raise Exception(\"INF values present in RELU BAck\")\n",
    "        return dZ\n",
    "        \n",
    "    def categorical_cross_entropy_backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs_loss = self.dinputs / samples\n",
    "        if(np.isnan(np.sum(self.dinputs))):\n",
    "            raise Exception(\"NaN values present in Softmax Back\")\n",
    "        elif(np.isinf(np.sum(self.dinputs))):\n",
    "            raise Exception(\"INF values present in Softmax_back\")\n",
    "        return self.dinputs\n",
    "    \n",
    "    def softmax_categorical_cross_entropy_combined_backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        #handling Ohe values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs_combined = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs_combined[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs_combined = self.dinputs_combined / samples\n",
    "        if(np.isnan(np.sum(self.dinputs_combined))):\n",
    "            raise Exception(\"NaN values present in Softmax Back\")\n",
    "        elif(np.isinf(np.sum(self.dinputs_combined))):\n",
    "            raise Exception(\"INF values present in Softmax_back\")\n",
    "       \n",
    "        return self.dinputs_combined\n",
    "        \n",
    "    \n",
    "    def compute_loss(self,y_pred, y_true):\n",
    "        sample_losses = self.categorical_cross_entropy(y_pred, y_true)\n",
    "        loss = np.mean(sample_losses)\n",
    "        return loss\n",
    "    \n",
    "    def compute_loss_with_l2(self,y_pred, y_true, lambd = 0.5):\n",
    "        m = 10\n",
    "        sample_losses = self.categorical_cross_entropy(y_pred, y_true)\n",
    "        L2_regularization_cost = (lambd/(2*m))*(np.sum(np.square(self.weights1) + np.sum(np.square(self.weights2) + np.sum(np.square(self.weights3)))))\n",
    "        loss = np.mean(sample_losses) \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        self.X = X\n",
    "        self.output1     = self.forward(self.X, self.weights1, self.biases1)\n",
    "        self.output1_act = self.ReLU(self.output1)\n",
    "        self.output2     = self.forward(self.output1_act, self.weights2, self.biases2)\n",
    "        self.output2_act = self.ReLU(self.output2)\n",
    "        self.output3     = self.forward(self.output2_act, self.weights3, self.biases3)\n",
    "        self.output3_act = self.Softmax(self.output3)\n",
    "#         print(\"Softmax SUM\", np.sum(self.output3_act, axis = 1))\n",
    "        if(np.isnan(np.sum(self.output3_act))):\n",
    "            raise Exception(\"NaN values present in data\")\n",
    "        elif(np.isinf(np.sum(self.output3_act))):\n",
    "            raise Exception(\"INF values present in data\")\n",
    "        \n",
    "        \n",
    "    def check_inf(self):\n",
    "        check_weights = np.any(np.isinf(self.weights1)) or np.any(np.isinf(self.weights2)) or np.any(np.isinf(self.weights3))\n",
    "        check_bias    = np.any(np.isinf(self.biases1)) or np.any(np.isinf(self.biases2)) or np.any(np.isinf(self.biases3))\n",
    "        return (check_weights or check_bias)\n",
    "    \n",
    "    def bgd(self, X, y, learning_rate= 0.1, iteration = 10000):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        for i in range(iteration):\n",
    "            self.forward_pass(self.X)\n",
    "            predictions = np.argmax(self.output3_act, axis=1)\n",
    "            \n",
    "            \n",
    "            gradient_output3_act                  = self.softmax_categorical_cross_entropy_combined_backward(self.output3_act, self.y)\n",
    "            gradient_output3, gradient_input3     = self.linear_backward(self.output2,self.weights3,gradient_output3_act)\n",
    "            gradient_output2_act                  = self.ReLU_backward(gradient_input3, self.output2)\n",
    "            gradient_output2, gradient_input2     = self.linear_backward(self.output1, self.weights2, gradient_output2_act)\n",
    "            gradient_output1_act                  = self.ReLU_backward(gradient_input2, self.output1)\n",
    "            gradient_output1, gradient_input1     = self.linear_backward(self.X, self.weights1, gradient_output1_act)\n",
    "            \n",
    "            self.weights3  = self.weights3 - learning_rate * gradient_output3\n",
    "            self.weights2  = self.weights2 - learning_rate * gradient_output2\n",
    "            self.weights1  = self.weights1 - learning_rate * gradient_output1\n",
    "            assert np.sum(gradient_output1) != np.nan, \"The gradient has nan\"\n",
    "            assert np.sum(gradient_output1) != np.inf, \"The gradient has inf\"\n",
    "            if i%100 == 0:\n",
    "\n",
    "                loss = self.compute_loss(self.output3_act, y)\n",
    "                self.accuracy = np.mean(predictions==self.y)\n",
    "                if(self.accuracy > 99.0):\n",
    "                    break\n",
    "                print(f' Training Loss after a iteration {i}:{loss} || Accuracy: {self.accuracy * 100}')\n",
    "                \n",
    "    def bgd_with_l2(self,X, y, learning_rate= 0.01, iteration = 10000):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.loss_list = []\n",
    "        self.acc_list = []\n",
    "        for i in range(iteration):\n",
    "            self.forward_pass(self.X)\n",
    "            predictions = np.argmax(self.output3_act, axis=1)\n",
    "            gradient_output3_act                  = self.softmax_categorical_cross_entropy_combined_backward(self.output3_act, self.y)\n",
    "#             print(gradient_output3_act)\n",
    "            gradient_output3, gradient_input3     = self.linear_backward_with_l2(self.output2,self.weights3,gradient_output3_act)\n",
    "            gradient_output2_act                  = self.ReLU_backward(gradient_input3, self.output2)\n",
    "            gradient_output2, gradient_input2     = self.linear_backward_with_l2(self.output1, self.weights2, gradient_output2_act)\n",
    "            gradient_output1_act                  = self.ReLU_backward(gradient_input2, self.output1)\n",
    "            gradient_output1, gradient_input1     = self.linear_backward_with_l2(self.X, self.weights1, gradient_output1_act)\n",
    "            \n",
    "            self.weights3  = self.weights3 - learning_rate * gradient_output3\n",
    "            self.weights2  = self.weights2 - learning_rate * gradient_output2\n",
    "            self.weights1  = self.weights1 - learning_rate * gradient_output1\n",
    "            assert np.sum(gradient_output1) != np.nan, \"The gradient has nan\"\n",
    "            assert np.sum(gradient_output1) != np.inf, \"The gradient has inf\"\n",
    "            loss = self.compute_loss_with_l2(self.output3_act, y)\n",
    "            self.accuracy = np.mean(predictions==self.y)\n",
    "            if i%100 == 0:\n",
    "                self.loss_list.append(loss)\n",
    "                self.acc_list.append(self.accuracy)\n",
    "                if(self.accuracy > .99):\n",
    "                    break\n",
    "                print(f'Loss after a iteration {i}:{loss} || Accuracy: {self.accuracy * 100}')\n",
    "        plt.plot(self.loss_list)\n",
    "        plt.title(\"Training loss of the model\")   \n",
    "    \n",
    "    def rand_mini_batches(self, X, Y, mini_batch_size = 128, seed=1):\n",
    "   \n",
    "        classes = Y.shape[0]\n",
    "        np.random.seed(seed)            \n",
    "        m = X.shape[0]                  # number of training examples\n",
    "        mini_batches = []\n",
    "\n",
    "    #     Shuffle (X, Y)\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[permutation, :]\n",
    "    #     shuffled_Y = Y[:, permutation].reshape((classes,m))\n",
    "        shuffled_Y = Y.iloc[permutation]\n",
    "\n",
    "    #     Partition (shuffled_X, shuffled_Y) except for the last batch\n",
    "        num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size \n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[ k * mini_batch_size : (k+1)*mini_batch_size, : ]\n",
    "    #         mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1)*mini_batch_size]\n",
    "            mini_batch_Y = shuffled_Y.iloc[k * mini_batch_size : (k+1)*mini_batch_size]\n",
    "\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        # Last batch (last mini-batch < mini_batch_size)\n",
    "        if m % mini_batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "    #         mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "            mini_batch_Y = shuffled_Y.iloc[ num_complete_minibatches * mini_batch_size : m]\n",
    "\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        return mini_batches   \n",
    "    \n",
    "    def mini_batch_gd(self, X, y, learning_rate= 0.01, iteration = 100, mini_batch_size = 128):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.loss_list = []\n",
    "        self.acc_list = []\n",
    "        for i in range(iteration):\n",
    "            seed = iteration\n",
    "            mini_batches= self.rand_mini_batches(X =  self.X_train, Y = self.y_train, mini_batch_size = mini_batch_size, seed=seed)\n",
    "            for mini_batch in mini_batches:\n",
    "                mini_batch_X , mini_batch_y = mini_batch\n",
    "                self.forward_pass(mini_batch_X)\n",
    "                predictions = np.argmax(self.output3_act, axis=1)\n",
    "                gradient_output3_act                  = self.softmax_categorical_cross_entropy_combined_backward(self.output3_act, mini_batch_y)\n",
    "    #             print(gradient_output3_act)\n",
    "                gradient_output3, gradient_input3     = self.linear_backward_with_l2(self.output2,self.weights3,gradient_output3_act)\n",
    "                gradient_output2_act                  = self.ReLU_backward(gradient_input3, self.output2)\n",
    "                gradient_output2, gradient_input2     = self.linear_backward_with_l2(self.output1, self.weights2, gradient_output2_act)\n",
    "                gradient_output1_act                  = self.ReLU_backward(gradient_input2, self.output1)\n",
    "                gradient_output1, gradient_input1     = self.linear_backward_with_l2(mini_batch_X, self.weights1, gradient_output1_act)\n",
    "\n",
    "                self.weights3  = self.weights3 - learning_rate * gradient_output3\n",
    "                self.weights2  = self.weights2 - learning_rate * gradient_output2\n",
    "                self.weights1  = self.weights1 - learning_rate * gradient_output1\n",
    "                assert np.sum(gradient_output1) != np.nan, \"The gradient has nan\"\n",
    "                assert np.sum(gradient_output1) != np.inf, \"The gradient has inf\"\n",
    "                loss = self.compute_loss_with_l2(self.output3_act, mini_batch_y)\n",
    "                self.accuracy = np.mean(predictions==mini_batch_y)\n",
    "                predictions_test = self.predict(self.X_val)\n",
    "                test_accuracy = np.mean(predictions_test == self.y_val)\n",
    "            self.loss_list.append(loss)\n",
    "            self.acc_list.append(self.accuracy)\n",
    "            print(f'Epoch({i+1}): Training Loss :{\"%.2f\"%loss} || Training Accuracy: {\"%.2f\"%(self.accuracy * 100)}%|| Validation Accuracy :{\"%.2f\"%(test_accuracy * 100)}% ')\n",
    "        plt.plot(self.loss_list)\n",
    "        plt.title(\"Training loss of the model\")\n",
    "        \n",
    "    def load_model(self, weights, biases = None):\n",
    "        self.weights1 = weights['1']\n",
    "        self.weights2 = weights['2']\n",
    "        self.weights3 = weights['3']\n",
    "        \n",
    "        self.biases1 = weights['b1']\n",
    "        self.biases2 = weights['b2']\n",
    "        self.biases3 = weights['b3']\n",
    "    \n",
    "    def save_model(self, filename = f'model'):\n",
    "        from datetime import date\n",
    "\n",
    "        today = date.today()\n",
    "\n",
    "        filename = f'{filename}_{self.accuracy}-{today}.pkl'\n",
    "        weights = {\n",
    "                    '1': self.weights1, '2': self.weights2, '3': self.weights3, \n",
    "                    'b1':self.biases1,'b2':self.biases2,'b3':self.biases3\n",
    "        }\n",
    "        pickle.dump(weights, open(filename, 'wb'))\n",
    "    \n",
    "    def fit(self, X,y, optimizer = 'bgd', iters = 10000, learning_rate = 0.001, batch_size = None, regularizer = 'l2'):\n",
    "        '''\n",
    "        A Method to fit the Model\n",
    "        X<pd.DataFrame> : The set of features\n",
    "        y<pd.DataFrame> : The target Labels\n",
    "        optimizer<str> : The optimizer for the model (bgd, mini_batch)\n",
    "        learning_rate<float>: The learning rate for the model\n",
    "        batch_size<int>(optional): 1 results in SGD, optional when optimizer = 'bgd' \n",
    "        regularizer<str>(optional) :values(None/'l2') The regularizer for the system \n",
    "        \n",
    "        '''\n",
    "        if (optimizer == 'bgd' and  regularizer is None) :\n",
    "            self.bgd(self, X, y, learning_rate= learning_rate, iteration = iteration)\n",
    "        elif (optimizer == 'bgd') and (regularizer == 'l2'):\n",
    "            self.bgd_with_l2(X = X, y = y, learning_rate = learning_rate, iteration = iteration)\n",
    "        elif (optimizer == 'mini_batch'):\n",
    "            if batch_size is not None:\n",
    "                self.mini_batch_gd(X = X, y= y, learning_rate= learning_rate, iteration = iters, mini_batch_size = batch_size)\n",
    "            else: \n",
    "                raise Exception(\"Specify the batch_size for Mini Batch Gradient Descent\")\n",
    "        else:\n",
    "            raise Exception(\"Invalid parameters passed !\")\n",
    "            \n",
    "    def get_validation_data(self, X_val, y_val):\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        wt1, wt2, wt3 = self.weights1.copy(), self.weights2.copy(), self.weights3.copy()\n",
    "        b1, b2, b3    = self.biases1.copy(), self.biases2.copy(), self.biases3.copy()\n",
    "        output1     = self.forward(X_test, wt1, b1)\n",
    "        output1_act = self.ReLU(output1)\n",
    "        output2     = self.forward(output1_act,  wt2, b2)\n",
    "        output2_act = self.ReLU(output2)\n",
    "        output3     = self.forward(output2_act,  wt3, b3)\n",
    "        output3_act = self.Softmax(output3)\n",
    "        prediction, prediction_prob = np.argmax(output3_act, axis=1), np.max(output3_act, axis=1)\n",
    "        preds = [(output3_act[i], i) for i in range(0,10)]\n",
    "        print(preds)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16fc187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch(1): Training Loss :2.30 || Training Accuracy: 15.62 || Validation Accuracy :9.96 \n",
      "Epoch(2): Training Loss :2.30 || Training Accuracy: 15.62 || Validation Accuracy :9.97 \n",
      "Epoch(3): Training Loss :2.30 || Training Accuracy: 12.50 || Validation Accuracy :11.35 \n",
      "Epoch(4): Training Loss :2.30 || Training Accuracy: 25.00 || Validation Accuracy :24.15 \n",
      "Epoch(5): Training Loss :2.21 || Training Accuracy: 12.50 || Validation Accuracy :14.64 \n",
      "Epoch(6): Training Loss :1.88 || Training Accuracy: 15.62 || Validation Accuracy :28.03 \n",
      "Epoch(7): Training Loss :1.53 || Training Accuracy: 40.62 || Validation Accuracy :48.91 \n",
      "Epoch(8): Training Loss :1.23 || Training Accuracy: 56.25 || Validation Accuracy :61.81 \n",
      "Epoch(9): Training Loss :1.03 || Training Accuracy: 65.62 || Validation Accuracy :67.82 \n",
      "Epoch(10): Training Loss :0.94 || Training Accuracy: 65.62 || Validation Accuracy :74.53 \n",
      "Epoch(11): Training Loss :0.83 || Training Accuracy: 71.88 || Validation Accuracy :78.46 \n",
      "Epoch(12): Training Loss :0.73 || Training Accuracy: 84.38 || Validation Accuracy :81.62 \n",
      "Epoch(13): Training Loss :0.63 || Training Accuracy: 81.25 || Validation Accuracy :84.05 \n",
      "Epoch(14): Training Loss :0.54 || Training Accuracy: 84.38 || Validation Accuracy :85.74 \n",
      "Epoch(15): Training Loss :0.48 || Training Accuracy: 90.62 || Validation Accuracy :86.86 \n",
      "Epoch(16): Training Loss :0.44 || Training Accuracy: 90.62 || Validation Accuracy :87.62 \n",
      "Epoch(17): Training Loss :0.41 || Training Accuracy: 90.62 || Validation Accuracy :88.32 \n",
      "Epoch(18): Training Loss :0.39 || Training Accuracy: 93.75 || Validation Accuracy :88.76 \n",
      "Epoch(19): Training Loss :0.37 || Training Accuracy: 93.75 || Validation Accuracy :89.31 \n",
      "Epoch(20): Training Loss :0.36 || Training Accuracy: 93.75 || Validation Accuracy :89.66 \n",
      "Epoch(21): Training Loss :0.35 || Training Accuracy: 93.75 || Validation Accuracy :89.93 \n",
      "Epoch(22): Training Loss :0.34 || Training Accuracy: 93.75 || Validation Accuracy :90.38 \n",
      "Epoch(23): Training Loss :0.33 || Training Accuracy: 93.75 || Validation Accuracy :90.58 \n",
      "Epoch(24): Training Loss :0.32 || Training Accuracy: 93.75 || Validation Accuracy :90.89 \n",
      "Epoch(25): Training Loss :0.31 || Training Accuracy: 93.75 || Validation Accuracy :91.08 \n",
      "Epoch(26): Training Loss :0.30 || Training Accuracy: 90.62 || Validation Accuracy :91.28 \n",
      "Epoch(27): Training Loss :0.30 || Training Accuracy: 90.62 || Validation Accuracy :91.45 \n",
      "Epoch(28): Training Loss :0.29 || Training Accuracy: 90.62 || Validation Accuracy :91.61 \n",
      "Epoch(29): Training Loss :0.29 || Training Accuracy: 90.62 || Validation Accuracy :91.73 \n",
      "Epoch(30): Training Loss :0.29 || Training Accuracy: 90.62 || Validation Accuracy :91.88 \n",
      "Epoch(31): Training Loss :0.29 || Training Accuracy: 90.62 || Validation Accuracy :91.98 \n",
      "Epoch(32): Training Loss :0.29 || Training Accuracy: 90.62 || Validation Accuracy :92.12 \n",
      "Epoch(33): Training Loss :0.28 || Training Accuracy: 90.62 || Validation Accuracy :92.15 \n",
      "Epoch(34): Training Loss :0.28 || Training Accuracy: 90.62 || Validation Accuracy :92.24 \n",
      "Epoch(35): Training Loss :0.28 || Training Accuracy: 90.62 || Validation Accuracy :92.35 \n",
      "Epoch(36): Training Loss :0.27 || Training Accuracy: 90.62 || Validation Accuracy :92.46 \n",
      "Epoch(37): Training Loss :0.27 || Training Accuracy: 90.62 || Validation Accuracy :92.53 \n",
      "Epoch(38): Training Loss :0.27 || Training Accuracy: 90.62 || Validation Accuracy :92.61 \n",
      "Epoch(39): Training Loss :0.27 || Training Accuracy: 90.62 || Validation Accuracy :92.68 \n",
      "Epoch(40): Training Loss :0.26 || Training Accuracy: 90.62 || Validation Accuracy :92.69 \n",
      "Epoch(41): Training Loss :0.26 || Training Accuracy: 90.62 || Validation Accuracy :92.79 \n",
      "Epoch(42): Training Loss :0.26 || Training Accuracy: 90.62 || Validation Accuracy :92.91 \n",
      "Epoch(43): Training Loss :0.25 || Training Accuracy: 90.62 || Validation Accuracy :92.92 \n",
      "Epoch(44): Training Loss :0.25 || Training Accuracy: 90.62 || Validation Accuracy :92.95 \n",
      "Epoch(45): Training Loss :0.25 || Training Accuracy: 90.62 || Validation Accuracy :92.96 \n",
      "Epoch(46): Training Loss :0.25 || Training Accuracy: 90.62 || Validation Accuracy :92.97 \n",
      "Epoch(47): Training Loss :0.24 || Training Accuracy: 90.62 || Validation Accuracy :93.02 \n",
      "Epoch(48): Training Loss :0.24 || Training Accuracy: 90.62 || Validation Accuracy :93.05 \n",
      "Epoch(49): Training Loss :0.23 || Training Accuracy: 90.62 || Validation Accuracy :93.09 \n",
      "Epoch(50): Training Loss :0.23 || Training Accuracy: 93.75 || Validation Accuracy :93.12 \n",
      "Epoch(51): Training Loss :0.23 || Training Accuracy: 93.75 || Validation Accuracy :93.14 \n",
      "Epoch(52): Training Loss :0.22 || Training Accuracy: 93.75 || Validation Accuracy :93.21 \n",
      "Epoch(53): Training Loss :0.22 || Training Accuracy: 93.75 || Validation Accuracy :93.22 \n",
      "Epoch(54): Training Loss :0.22 || Training Accuracy: 93.75 || Validation Accuracy :93.21 \n",
      "Epoch(55): Training Loss :0.22 || Training Accuracy: 93.75 || Validation Accuracy :93.30 \n",
      "Epoch(56): Training Loss :0.21 || Training Accuracy: 93.75 || Validation Accuracy :93.32 \n"
     ]
    }
   ],
   "source": [
    "model  = Classifier(1024, [32,32, 10])\n",
    "model.get_validation_data(X_val = X_test, y_val= y_test)\n",
    "model.fit(X_train, y_train, optimizer = 'mini_batch' , iters = 300, batch_size = 128, learning_rate = 0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ebb227",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"SGD_Model_EPOCH_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c876e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e95dcd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20569    2\n",
       "6186     0\n",
       "11417    1\n",
       "7064     0\n",
       "59855    7\n",
       "        ..\n",
       "21211    2\n",
       "27344    3\n",
       "7100     0\n",
       "55159    6\n",
       "52081    6\n",
       "Name: label, Length: 17000, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b8f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b5557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df16d48a",
   "metadata": {},
   "source": [
    "# Check Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e4d76b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score,confusion_matrix, ConfusionMatrixDisplay\n",
    "acc = accuracy_score(y_test.values, pr)\n",
    "rec = recall_score(y_test.values, pr, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1023aade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1649,    0,    0,    0,    1,    0,    1,   11,    7,    5],\n",
       "       [   2, 1682,    3,    2,    2,    3,    4,    0,    0,    5],\n",
       "       [   0,    9, 1472,   85,    4,   24,   22,    2,    6,    3],\n",
       "       [   0,    6,  103, 1575,    3,   25,   18,    4,    0,    2],\n",
       "       [   0,    9,    1,    2, 1705,   18,    3,    4,   10,    8],\n",
       "       [   0,    2,   27,   16,   26, 1590,    9,    5,    2,    1],\n",
       "       [   7,   11,    5,   18,    9,    7, 1583,   28,    8,   18],\n",
       "       [  24,    0,    2,    4,    2,   15,   27, 1661,    2,    0],\n",
       "       [  11,    0,    2,    0,    2,    1,   14,    1, 1624,   11],\n",
       "       [   2,   17,    2,    3,    5,    0,   13,    3,    7, 1673]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_test.values, pr )\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "407271c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.953639785345603"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99727d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
