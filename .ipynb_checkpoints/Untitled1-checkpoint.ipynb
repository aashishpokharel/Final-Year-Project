{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463efb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66fbab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = {\n",
    "                'digit_0' : 0,\n",
    "                'digit_1' : 1,\n",
    "                'digit_2' : 2,\n",
    "                'digit_3' : 3,\n",
    "                'digit_4' : 4,\n",
    "                'digit_5' : 5,\n",
    "                'digit_6' : 6,\n",
    "                'digit_7' : 7,\n",
    "                'digit_8' : 8,\n",
    "                'digit_9' : 9,\n",
    "}\n",
    "train_data = pd.read_csv('./dataset/train_digits_data.csv')\n",
    "test_data  = pd.read_csv('./dataset/test_digits_data.csv')\n",
    "X_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data.iloc[:, -1]\n",
    "y_train = y_train.replace(charset)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size = .01)\n",
    "\n",
    "X_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ecd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "X_train = mm.fit_transform(X_train)\n",
    "X_dev   = mm.fit_transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1928cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Before Processing\t After Processing\n",
      "=================================================================\n",
      "Training Set Images:\t(17000, 1025)\t\t(16830, 1024)\n",
      "Training Set Labels:\t(17000,)\t\t(16830,)\n",
      "Dev Set Images:\t\t(170, 1024)\t\t(170, 1024)\n",
      "Dev Set Labels:\t\t(170,)\t\t\t(170,)\n",
      "Test Set Images:\t(3000, 1025)\t\t(3000, 1024)\n",
      "Test Set Labels:\t(3000,)\t\t\t(3000,)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Data\\t\\t\\t\",\"Before Processing\\t\",\"After Processing\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(train_data.shape)+\"\\t\\t\"+ str(X_train.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(train_data.iloc[:, -1].shape)+\"\\t\\t\"+ str(y_train.shape))\n",
    "print(\"Dev Set Images:\\t\\t\" + str(X_dev.shape)+\"\\t\\t\"+ str(X_dev.shape))\n",
    "print(\"Dev Set Labels:\\t\\t\" + str(y_dev.shape)+\"\\t\\t\\t\"+ str(y_dev.shape))\n",
    "print(\"Test Set Images:\\t\" + str(test_data.shape)+\"\\t\\t\"+ str(X_test.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(test_data.iloc[:, -1].shape)+\"\\t\\t\\t\"+ str(test_data.iloc[:, -1].shape))\n",
    "print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba481ee",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b870645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "        Compute the ReLU activation of Z\n",
    "        \n",
    "        Argument:\n",
    "            - Z -- Array of the Sum of the product of Weights and input\n",
    "        \n",
    "        Returns:\n",
    "            - A -- Array of Activation obtained by applying ReLU function. same size as that of Z\n",
    "    \"\"\"\n",
    "    A = np.maximum(0.0,Z)\n",
    "    \n",
    "    cache = Z\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b91f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(dA, cache):\n",
    "    \"\"\"\n",
    "        Compute the gradient of dA\n",
    "        \n",
    "        Arguments:\n",
    "            - dA -- Array of the gradient of activation of the previous layer\n",
    "            - cache -- list of other useful variables like Z\n",
    "            \n",
    "        Returns:\n",
    "            - dZ -- array of gradient/derivative of the dA, Same size of dA\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    dZ[Z < 0] = 0\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "538a61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "        Compute the softmax activtion of Z\n",
    "        \n",
    "        Argument:\n",
    "            - Z -- Array of the Sum of the product of Weights and input\n",
    "        \n",
    "        Returns:\n",
    "            - A -- Array of Activation obtained by applying Softmax function. same size as that of Z\n",
    "    \"\"\"\n",
    "    shift = Z - np.max(Z) #Avoiding underflow or overflow errors due to floating point instability in softmax\n",
    "    t = np.exp(shift)\n",
    "    A = np.divide(t,np.sum(t,axis = 0))\n",
    "    \n",
    "    cache = Z\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "179eaaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers():\n",
    "    \"\"\"\n",
    "        Initializes the layers of NN with certain neural units in each layers\n",
    "        \n",
    "        Returns:\n",
    "            - layer_dim -- list of the units of each layer of the network\n",
    "    \"\"\"\n",
    "    layers_dim = [1024,32,32,10]\n",
    "    return layers_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f4d995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layers_dim):\n",
    "    \"\"\"\n",
    "        Initializes the parameters of the Neural Network\n",
    "        \n",
    "        Argument:\n",
    "            - layers_dim -- Layer Dimensions of the NN\n",
    "        \n",
    "        Returns:\n",
    "            - params -- Dictionary of the initialized parameters: Weights and biases\n",
    "    \"\"\"\n",
    "    L = len(layers_dim)\n",
    "    params = {}\n",
    "        \n",
    "    for l in range(1,L):\n",
    "        params['W' + str(l)] = np.random.randn(layers_dim[l],layers_dim[l-1]) *0.01\n",
    "        params['b' + str(l)] = np.zeros((layers_dim[l],1))\n",
    "     \n",
    "        assert(params['W' + str(l)].shape == (layers_dim[l],layers_dim[l-1]))\n",
    "        assert(params['b' + str(l)].shape == (layers_dim[l],1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfff95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hyperParams(alpha = 0.01, num_iteration = 5000):\n",
    "    \"\"\"\n",
    "        Initializes the hyper parameters\n",
    "        \n",
    "        Arguments:\n",
    "            - alpha -- learning rate\n",
    "            - num_iteration -- number of iteration the gradient descent will run\n",
    "        Returns:\n",
    "            - Dictionary of hyper parameters\n",
    "        \n",
    "    \"\"\"\n",
    "    hyperParams = {}\n",
    "    hyperParams['learning_rate'] = alpha\n",
    "    hyperParams['num_iterations'] = num_iteration\n",
    "    \n",
    "    \n",
    "    return hyperParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fcfe08",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b597ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_sum(A,W,b):\n",
    "    \"\"\"\n",
    "        Calculates the forward sum \n",
    "        \n",
    "        Arguments:\n",
    "            - A -- array of activation from the previous layer\n",
    "            - W -- weights of the current layer\n",
    "            - b -- bias of the current layer\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    cache = (A,W,b)\n",
    "    assert(Z.shape == (W.shape[0],Z.shape[1]))\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "010dadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_activation(A,W,b,activation):\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        Z, sum_cache = forward_sum(A,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    if activation == 'softmax':\n",
    "        Z, sum_cache = forward_sum(A,W,b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    cache = (sum_cache,activation_cache)\n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4122b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = forward_activation(A_prev,parameters['W' + str(l)],parameters['b' + str(l)],activation='relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = forward_activation(A,parameters['W' + str(L)],parameters['b' + str(L)],activation='softmax')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (10,X.shape[1]))\n",
    "    \n",
    "    return AL,caches\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53f1c4",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccf35754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL,Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = -(1./m) * np.sum(np.sum(np.multiply(Y,np.log(AL)), axis = 0,keepdims=True))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # Making sure your cost's shape is not returned as ndarray\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edcbe07",
   "metadata": {},
   "source": [
    "# Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e16826ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_grad(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m) * np.dot(dZ,A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis = 1, keepdims=True )\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35aca639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_activation(dA,cache,activation):\n",
    "    sum_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_grad(dA,activation_cache)\n",
    "        dA_prev, dW, db = backward_grad(dZ, sum_cache)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        dZ = dA\n",
    "        dA_prev, dW, db = backward_grad(dA, sum_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4d89e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(AL, Y,caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    dA = np.subtract(AL,Y)\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = backward_activation(dA, current_cache, activation = 'softmax')\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_activation(grads[\"dA\" + str(l + 1)], current_cache, activation = 'relu')\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc63ca6",
   "metadata": {},
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a872c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * grads[\"dW\" + str(l+1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * grads[\"db\" + str(l+1)])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45505b9a",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36281211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,y,parameters):\n",
    "    m = y.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    probas, caches = forward_prop(X, parameters)\n",
    "   \n",
    "    assert(probas.shape == y.shape)\n",
    "        \n",
    "    predicted_labels = np.argmax(probas,axis=0).reshape(1,probas.shape[1])\n",
    "    predicted_prob = np.max(probas,axis = 0).reshape(1,m)\n",
    "    \n",
    "    Y = np.argmax(y,axis=0).reshape(1,y.shape[1])\n",
    "    \n",
    "    true_prediction = np.equal(predicted_labels,Y)\n",
    "    \n",
    "    num_correct_labels = np.sum(true_prediction)\n",
    "    accuracy = num_correct_labels / m\n",
    "        \n",
    "    return predicted_labels, predicted_prob, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4579675a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98254b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, Y_train, X_dev, Y_dev, layers_dim, hyperParams):\n",
    "\n",
    "    learning_rate = hyperParams['learning_rate']\n",
    "    num_iterations = hyperParams['num_iterations']\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    costs = []      # keep track of cost\n",
    "    train_accs = []  # keep track of training accuracy\n",
    "    val_accs = []     # keep track of Validation accuracy\n",
    "    \n",
    "    parameters = init_params(layers_dim)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, caches = forward_prop(X_train, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y_train)\n",
    "    \n",
    "        grads = backward_prop(AL, Y_train, caches)\n",
    " \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        _,_,train_acc = predict(X_train, Y_train,parameters)\n",
    "        _,_,val_acc= predict(X_dev, Y_dev,parameters)        \n",
    "        \n",
    "        if i == 0 or (i+1) % 200 == 0:\n",
    "            print (\"Iteration: %d == Cost: %f || Training acc: %f || Val acc: %f\"%(i,cost,train_acc,val_acc))\n",
    "        if i == 0 or (i+1) % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            \n",
    "    visualize_results(costs, attr_type='costs')  \n",
    "    visualize_results(train_accs, attr_type='train_accs')       \n",
    "    visualize_results(val_accs, attr_type='val_accs')       \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea19642",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e77f5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(attr, attr_type):\n",
    "    \n",
    "    plt.plot(np.squeeze(attr))\n",
    "    if attr_type == 'costs':\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.title(\"Cost\")\n",
    "        \n",
    "    elif attr_type == 'train_accs':\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.title(\"Training Accuracy\")\n",
    "        \n",
    "    elif attr_type == 'val_accs':\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.title(\"Validation Accuracy\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Dataset set must be training or dev or test set\")\n",
    "        \n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480bed8e",
   "metadata": {},
   "source": [
    "# Baselining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b128f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 == Cost: 2.297805 || Training acc: 0.242127 || Val acc: 0.223529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10578/817495027.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A = np.divide(t,np.sum(t,axis = 0))\n",
      "/tmp/ipykernel_10578/558296111.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -(1./m) * np.sum(np.sum(np.multiply(Y,np.log(AL)), axis = 0,keepdims=True))\n",
      "/tmp/ipykernel_10578/558296111.py:4: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -(1./m) * np.sum(np.sum(np.multiply(Y,np.log(AL)), axis = 0,keepdims=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 199 == Cost: nan || Training acc: 0.100238 || Val acc: 0.076471\n",
      "Iteration: 399 == Cost: nan || Training acc: 0.100238 || Val acc: 0.076471\n",
      "Iteration: 599 == Cost: nan || Training acc: 0.100238 || Val acc: 0.076471\n"
     ]
    }
   ],
   "source": [
    "hyperParams = init_hyperParams(alpha = 0.1,num_iteration = 10000)\n",
    "layers_dim = init_layers()\n",
    "parameters = train(X_train.T, y_train_e.values.T,X_dev.T, y_dev_e.values.T,layers_dim, hyperParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "511640b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16830, 1024)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "674efb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_e = pd.get_dummies(y_train)\n",
    "y_dev_e   = pd.get_dummies(y_dev)\n",
    "y_train   = pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25f64c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_e.values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dee095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
